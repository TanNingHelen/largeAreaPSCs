import numpy as np
import os
import joblib
from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import matplotlib

matplotlib.use('Agg')
import matplotlib.pyplot as plt
import pandas as pd
import warnings

warnings.filterwarnings('ignore')

# åˆ›å»ºç›®å½•ç»“æ„
os.makedirs("models", exist_ok=True)
os.makedirs("img", exist_ok=True)

# åŠ è½½æ•°æ®
df = pd.read_excel("FinalData.xlsx")

# é€‰æ‹©ç‰¹å®šç‰¹å¾ï¼šCs, MA, FA, I, Br
feature_columns = ['Cs', 'MA', 'FA', 'Pb','I', 'Br','Cl']
target_column = 'Bandgap'

# æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨
missing_features = [col for col in feature_columns if col not in df.columns]
if missing_features:
    print(f"âŒ ç¼ºå¤±ç‰¹å¾: {missing_features}")
    exit()

print("âœ… æ‰€æœ‰å¿…éœ€ç‰¹å¾éƒ½å­˜åœ¨")

# å‡†å¤‡æ•°æ®
X = df[feature_columns]
y = df[target_column]

print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
print(f"ç‰¹å¾åˆ—è¡¨: {feature_columns}")
print(f"ç›®æ ‡å˜é‡: {target_column}")
print(f"Bandgapç»Ÿè®¡: æœ€å°å€¼={y.min():.4f}, æœ€å¤§å€¼={y.max():.4f}, å‡å€¼={y.mean():.4f}")

# æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
print("\n=== ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯ ===")
for col in feature_columns:
    print(f"{col}: æœ€å°å€¼={X[col].min():.4f}, æœ€å¤§å€¼={X[col].max():.4f}, å‡å€¼={X[col].mean():.4f}")

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=16)

# æ¨¡å‹è·¯å¾„
MODEL_PATH = "models/best_catboost_bandgap.cbm"


def calculate_metrics(y_true, y_pred):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    r = np.corrcoef(y_true, y_pred)[0, 1]
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    return r, r2, mae, rmse


def plot_feature_importance(model, feature_names):
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾"""
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': model.get_feature_importance()
    }).sort_values('importance', ascending=False)

    plt.figure(figsize=(10, 6))
    plt.barh(importance_df['feature'], importance_df['importance'])
    plt.xlabel('Feature Importance')
    plt.title('Feature Importance for Bandgap Prediction')
    plt.tight_layout()
    plt.savefig('img/feature_importance_bandgap_simple.png', dpi=300, bbox_inches='tight')
    plt.close()

    return importance_df


def plot_predictions(y_train_true, y_train_pred, y_test_true, y_test_pred):
    """ç»˜åˆ¶é¢„æµ‹ç»“æœå›¾"""
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.scatter(y_train_true, y_train_pred, alpha=0.6, color='blue', label='Training')
    plt.plot([y_train_true.min(), y_train_true.max()], [y_train_true.min(), y_train_true.max()], 'r--', linewidth=2)
    plt.xlabel('Actual Bandgap (eV)')
    plt.ylabel('Predicted Bandgap (eV)')
    plt.title('Training Set')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.scatter(y_test_true, y_test_pred, alpha=0.6, color='green', label='Test')
    plt.plot([y_test_true.min(), y_test_true.max()], [y_test_true.min(), y_test_true.max()], 'r--', linewidth=2)
    plt.xlabel('Actual Bandgap (eV)')
    plt.ylabel('Predicted Bandgap (eV)')
    plt.title('Test Set')
    plt.legend()

    plt.tight_layout()
    plt.savefig('img/bandgap_prediction_simple.png', dpi=300, bbox_inches='tight')
    plt.close()


# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
model_exists = os.path.exists(MODEL_PATH)
if model_exists:
    try:
        model = CatBoostRegressor()
        model.load_model(MODEL_PATH)
        print("âœ… åŠ è½½é¢„è®­ç»ƒçš„CatBoost Bandgapæ¨¡å‹...")
    except Exception as e:
        print(f"âŒ åŠ è½½ç°æœ‰æ¨¡å‹å¤±è´¥: {str(e)}ï¼Œé‡æ–°è®­ç»ƒ...")
        model_exists = False

if not model_exists:
    print("ğŸš€ è®­ç»ƒæ–°çš„CatBoost Bandgapæ¨¡å‹...")

    # ç®€åŒ–çš„å‚æ•°ç½‘æ ¼ï¼ˆç‰¹å¾å°‘ï¼Œä¸éœ€è¦å¤æ‚å‚æ•°ï¼‰
    param_grid = {
        'iterations': [300, 500, 800],
        'depth': [4, 5, 6],
        'learning_rate': [0.01, 0.05, 0.1],
        'l2_leaf_reg': [1, 3, 5],
        'random_strength': [1, 2],
    }

    # ä½¿ç”¨äº¤å‰éªŒè¯æ‰¾åˆ°æœ€ä½³å‚æ•°
    best_score = -np.inf
    best_params = None
    best_model = None

    print("ğŸ” è¿›è¡Œå‚æ•°æœç´¢...")

    for iterations in param_grid['iterations']:
        for depth in param_grid['depth']:
            for lr in param_grid['learning_rate']:
                for l2 in param_grid['l2_leaf_reg']:
                    for random_strength in param_grid['random_strength']:

                        model = CatBoostRegressor(
                            iterations=iterations,
                            depth=depth,
                            learning_rate=lr,
                            l2_leaf_reg=l2,
                            random_strength=random_strength,
                            loss_function='RMSE',
                            eval_metric='R2',
                            random_seed=42,
                            verbose=False,
                            thread_count=-1
                        )

                        # ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°
                        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
                        mean_score = cv_scores.mean()

                        if mean_score > best_score:
                            best_score = mean_score
                            best_params = {
                                'iterations': iterations,
                                'depth': depth,
                                'learning_rate': lr,
                                'l2_leaf_reg': l2,
                                'random_strength': random_strength
                            }
                            best_model = model

    # ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print(f"ğŸ¯ æœ€ä½³å‚æ•°: {best_params}")
    print(f"æœ€ä½³äº¤å‰éªŒè¯RÂ²: {best_score:.4f}")

    model = CatBoostRegressor(**best_params, random_seed=42, verbose=100)
    model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50, verbose=False)

    # ä¿å­˜æ¨¡å‹
    model.save_model(MODEL_PATH)
    print("âœ… æ¨¡å‹ä¿å­˜å®Œæˆ")

# é¢„æµ‹ç»“æœ
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# è®¡ç®—è¯„ä¼°æŒ‡æ ‡
train_r, train_r2, train_mae, train_rmse = calculate_metrics(y_train, y_train_pred)
test_r, test_r2, test_mae, test_rmse = calculate_metrics(y_test, y_test_pred)

print("\n" + "=" * 50)
print("=== æœ€ç»ˆæ¨¡å‹æ€§èƒ½ - Bandgapé¢„æµ‹ ===")
print("=" * 50)
print("\n=== è®­ç»ƒé›†æŒ‡æ ‡ ===")
print(f"R: {train_r:.4f}")
print(f"RÂ²: {train_r2:.4f}")
print(f"MAE: {train_mae:.4f} eV")
print(f"RMSE: {train_rmse:.4f} eV")

print("\n=== æµ‹è¯•é›†æŒ‡æ ‡ ===")
print(f"R: {test_r:.4f}")
print(f"RÂ²: {test_r2:.4f}")
print(f"MAE: {test_mae:.4f} eV")
print(f"RMSE: {test_rmse:.4f} eV")

# è®¡ç®—è¿‡æ‹Ÿåˆç¨‹åº¦
overfit_gap = train_r2 - test_r2
print(f"\n=== è¿‡æ‹Ÿåˆåˆ†æ ===")
print(f"è®­ç»ƒé›†-æµ‹è¯•é›†RÂ²å·®è·: {overfit_gap:.4f}")
if overfit_gap > 0.2:
    print("âš ï¸  æ£€æµ‹åˆ°æ˜æ˜¾è¿‡æ‹Ÿåˆ!")
elif overfit_gap > 0.1:
    print("â„¹ï¸  ä¸­ç­‰ç¨‹åº¦è¿‡æ‹Ÿåˆ")
else:
    print("âœ… æ³›åŒ–æ€§èƒ½è‰¯å¥½")

# ç‰¹å¾é‡è¦æ€§åˆ†æ
print("\n=== ç‰¹å¾é‡è¦æ€§åˆ†æ ===")
importance_df = plot_feature_importance(model, feature_columns)
print("ç‰¹å¾é‡è¦æ€§æ’åº:")
for idx, row in importance_df.iterrows():
    print(f"  {row['feature']}: {row['importance']:.4f}")

# ç»˜åˆ¶é¢„æµ‹ç»“æœ
plot_predictions(y_train, y_train_pred, y_test, y_test_pred)

# ä¿å­˜é¢„æµ‹ç»“æœ
print("\n=== ä¿å­˜é¢„æµ‹ç»“æœ ===")
results_df = pd.DataFrame({
    'Actual_Bandgap': pd.concat([y_train, y_test]),
    'Predicted_Bandgap': np.concatenate([y_train_pred, y_test_pred]),
    'Dataset': ['Training'] * len(y_train) + ['Test'] * len(y_test)
})

# æ·»åŠ ç‰¹å¾ä¿¡æ¯
for col in feature_columns:
    results_df[col] = pd.concat([X_train[col], X_test[col]]).values

results_df.to_csv('models/bandgap_predictions_simple.csv', index=False)
print(f"é¢„æµ‹ç»“æœä¿å­˜åˆ°: models/bandgap_predictions_simple.csv")

# ä¿å­˜æ¨¡å‹æ€§èƒ½ä¿¡æ¯
model_info = {
    'train_r2': train_r2,
    'test_r2': test_r2,
    'train_mae': train_mae,
    'test_mae': test_mae,
    'train_rmse': train_rmse,
    'test_rmse': test_rmse,
    'overfit_gap': overfit_gap,
    'features': ','.join(feature_columns)
}

model_info_df = pd.DataFrame([model_info])
model_info_df.to_csv('models/bandgap_model_performance_simple.csv', index=False)

print("\n=== æ¨¡å‹ä¿¡æ¯ ===")
print(f"Bandgapæ¨¡å‹ä¿å­˜åˆ°: {MODEL_PATH}")
print(f"ç‰¹å¾é‡è¦æ€§å›¾ä¿å­˜åˆ°: img/feature_importance_bandgap_simple.png")
print(f"é¢„æµ‹ç»“æœå›¾ä¿å­˜åˆ°: img/bandgap_prediction_simple.png")
print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(y_train)}")
print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(y_test)}")

# é¢å¤–ç»Ÿè®¡ä¿¡æ¯
print(f"\n=== Bandgapé¢„æµ‹ç»Ÿè®¡ ===")
print(f"å®é™…BandgapèŒƒå›´: {y.min():.4f} - {y.max():.4f} eV")
print(f"é¢„æµ‹BandgapèŒƒå›´: {results_df['Predicted_Bandgap'].min():.4f} - {results_df['Predicted_Bandgap'].max():.4f} eV")
print(f"æµ‹è¯•é›†MAEç›¸å¯¹è¯¯å·®: {test_mae / y.mean() * 100:.2f}%")

print("\nğŸ‰ Bandgapé¢„æµ‹æ¨¡å‹è®­ç»ƒå®Œæˆ!")