import pandas as pd
import numpy as np
import warnings
import os
from datetime import datetime

warnings.filterwarnings('ignore')


class OffsetBasedOptimizer:
    def __init__(self, data_path="FinalData.xlsx",
                 bestnip_path="bestnip.xlsx",
                 min_improvement=0.1,  # æœ€å°æ”¹è¿›å€¼
                 max_improvement=1.0):  # æœ€å¤§æ”¹è¿›å€¼
        self.data_path = data_path
        self.bestnip_path = bestnip_path
        self.min_improvement = min_improvement
        self.max_improvement = max_improvement

        # è¦ä¼˜åŒ–çš„ç‰¹å¾
        self.target_features = [
            'ETL_Passivator',
            'HTL_Passivator',
            'Precursor_Solution_Addictive'
        ]

        # åŠ è½½æ•°æ®
        self.df = None
        self.bestnip_records = None
        self.mapping_df = None
        self.original_mapping = {}  # ç¼–ç åˆ°åŸå§‹å€¼çš„æ˜ å°„

        # ç¼“å­˜ç‰¹å¾åˆ†æç»“æœ
        self.feature_impact_cache = {}

        # ç»“æœå­˜å‚¨
        self.optimization_results = {}

        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = 'offset_optimization_results'
        os.makedirs(self.results_dir, exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # åŠ è½½æ•°æ®
        self.load_data()

    def load_data(self):
        """åŠ è½½æ‰€æœ‰å¿…è¦çš„æ•°æ®"""
        print("ğŸ“‚ åŠ è½½æ•°æ®...")

        # åŠ è½½bestnip.xlsxä¸­çš„è®°å½•
        try:
            self.bestnip_records = pd.read_excel(self.bestnip_path)
            print(f"âœ… BestNIPè®°å½•åŠ è½½æˆåŠŸ: {len(self.bestnip_records)} æ¡")

            # æ˜¾ç¤ºè®°å½•çš„PCEå€¼
            print(f"\nğŸ“Š BestNIPè®°å½•PCEå€¼:")
            for idx, row in self.bestnip_records.iterrows():
                record_id = row.get('Record', f'Record_{idx + 1}')
                pce = row.get('PCE', 'N/A')
                print(f"  è®°å½• {idx + 1} (ID: {record_id}): PCE = {pce}%")

        except Exception as e:
            print(f"âŒ åŠ è½½BestNIPè®°å½•å¤±è´¥: {e}")
            raise

        # åŠ è½½æ•°æ®åº“
        try:
            self.df = pd.read_excel(self.data_path)
            print(f"âœ… æ•°æ®åº“åŠ è½½æˆåŠŸ: {len(self.df)} æ¡è®°å½•")

        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®åº“å¤±è´¥: {e}")
            raise

        # åŠ è½½æ˜ å°„æ–‡ä»¶
        self.load_mapping_file()

        # åˆ†ææ•°æ®åº“ä¸­çš„PCEåˆ†å¸ƒ
        self.analyze_pce_distribution()

    def load_mapping_file(self):
        """åŠ è½½æ˜ å°„æ–‡ä»¶ï¼Œæ„å»ºåŒå‘æ˜ å°„"""
        mapping_paths = [
            'label_mappings/full_mapping_summary.csv',
            '../label_mappings/full_mapping_summary.csv',
            './label_mappings/full_mapping_summary.csv'
        ]

        for path in mapping_paths:
            if os.path.exists(path):
                try:
                    self.mapping_df = pd.read_csv(path)
                    print(f"âœ… æ˜ å°„æ–‡ä»¶åŠ è½½æˆåŠŸ: {path}")

                    # æ„å»ºç¼–ç åˆ°åŸå§‹å€¼çš„æ˜ å°„
                    self.original_mapping = {}
                    for feature in self.target_features:
                        feature_mapping = self.mapping_df[self.mapping_df['Feature'] == feature]
                        if len(feature_mapping) > 0:
                            encoded_to_original = {}
                            for _, row in feature_mapping.iterrows():
                                encoded_value = row['Encoded']
                                original_value = row['Original']
                                if pd.isna(original_value):
                                    original_value = ''
                                encoded_to_original[encoded_value] = original_value

                            self.original_mapping[feature] = encoded_to_original
                            print(f"  ç‰¹å¾ '{feature}' æœ‰ {len(encoded_to_original)} ä¸ªæ˜ å°„")

                    return

                except Exception as e:
                    print(f"âŒ åŠ è½½æ˜ å°„æ–‡ä»¶å¤±è´¥ {path}: {e}")

        print("âš ï¸ æœªæ‰¾åˆ°æ˜ å°„æ–‡ä»¶")

    def analyze_pce_distribution(self):
        """åˆ†ææ•°æ®åº“ä¸­çš„PCEåˆ†å¸ƒ"""
        print(f"\nğŸ“Š æ•°æ®åº“PCEåˆ†æ:")
        print(f"  å¹³å‡å€¼: {self.df['PCE'].mean():.2f}%")
        print(f"  ä¸­ä½æ•°: {self.df['PCE'].median():.2f}%")
        print(f"  æœ€å¤§å€¼: {self.df['PCE'].max():.2f}%")
        print(f"  æœ€å°å€¼: {self.df['PCE'].min():.2f}%")
        print(f"  æ ‡å‡†å·®: {self.df['PCE'].std():.2f}%")

        # è¯†åˆ«é«˜PCEæ•°æ®ï¼ˆå‰20%ï¼‰
        high_pce_threshold = self.df['PCE'].quantile(0.8)
        high_pce_df = self.df[self.df['PCE'] >= high_pce_threshold]
        print(f"  é«˜PCEé˜ˆå€¼ï¼ˆå‰20%ï¼‰: {high_pce_threshold:.2f}%")
        print(f"  é«˜PCEè®°å½•æ•°: {len(high_pce_df)}")

    def decode_value(self, feature_name, encoded_value):
        """å°†ç¼–ç å€¼è§£ç ä¸ºåŸå§‹å€¼"""
        if feature_name in self.original_mapping:
            # å¤„ç†ç©ºå€¼
            if pd.isna(encoded_value) or encoded_value == '':
                return ''

            # å°è¯•è½¬æ¢ä¸ºæ•´æ•°
            try:
                if isinstance(encoded_value, str):
                    encoded_int = int(float(encoded_value))
                else:
                    encoded_int = int(encoded_value)

                original_value = self.original_mapping[feature_name].get(encoded_int, str(encoded_value))
                return original_value
            except:
                return str(encoded_value)
        return str(encoded_value)

    def analyze_feature_impact(self, feature_name):
        """åˆ†æç‰¹å¾å–å€¼å¯¹PCEçš„å½±å“ï¼Œä½¿ç”¨ç¼“å­˜æé«˜æ•ˆç‡"""
        if feature_name in self.feature_impact_cache:
            return self.feature_impact_cache[feature_name]

        if feature_name not in self.df.columns:
            return {}

        # ç­›é€‰æ‰ç©ºå€¼
        valid_data = self.df[self.df[feature_name].notna()].copy()

        if len(valid_data) == 0:
            return {}

        # å¯¹æ¯ä¸ªå–å€¼è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        impact_results = {}

        # è®¡ç®—æ€»ä½“å¹³å‡PCE
        overall_mean = valid_data['PCE'].mean()

        for value in valid_data[feature_name].unique():
            # ç­›é€‰è¯¥å–å€¼çš„è®°å½•
            value_data = valid_data[valid_data[feature_name] == value]

            if len(value_data) > 0:
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                pce_mean = value_data['PCE'].mean()
                pce_std = value_data['PCE'].std()
                count = len(value_data)

                # è®¡ç®—ä¸æ€»ä½“å¹³å‡çš„ç›¸å¯¹è¡¨ç°
                relative_performance = pce_mean - overall_mean

                # è§£ç åŸå§‹å€¼
                original_value = self.decode_value(feature_name, value)

                impact_results[value] = {
                    'original_value': original_value,
                    'encoded_value': value,
                    'pce_mean': pce_mean,
                    'pce_std': pce_std,
                    'count': count,
                    'relative_performance': relative_performance,
                    'display': f"{original_value} ({value})"
                }

        # ç¼“å­˜ç»“æœ
        self.feature_impact_cache[feature_name] = impact_results

        return impact_results

    def calculate_offset_predictions(self, original_pce, feature_impact, feature_name, original_encoded, record_id):
        """åŸºäºåç§»è®¡ç®—é¢„æµ‹PCEï¼Œè€ƒè™‘åŸå§‹è®°å½•çš„PCEæ°´å¹³"""
        predictions = []

        # è·å–å½“å‰ç‰¹å¾çš„æ€»ä½“ä¿¡æ¯
        overall_mean = self.df['PCE'].mean()
        overall_std = self.df['PCE'].std()

        for encoded_value, impact_info in feature_impact.items():
            # è·³è¿‡åŸå§‹å€¼æœ¬èº«
            if str(encoded_value) == str(original_encoded):
                continue

            # è·å–è¯¥å–å€¼çš„å¹³å‡PCE
            value_mean = impact_info['pce_mean']
            value_std = impact_info['pce_std']
            count = impact_info['count']

            # è®¡ç®—æ”¹è¿›å€¼ - åŸºäºåŸå§‹è®°å½•çš„PCEæ°´å¹³
            # æ–¹æ³•1ï¼šå¦‚æœåŸå§‹PCEä½äºè¯¥å–å€¼å¹³å‡PCEï¼Œåˆ™é¢„æµ‹ä¸ºè¯¥å–å€¼å¹³å‡PCE
            # æ–¹æ³•2ï¼šåŸºäºåŸå§‹PCEä¸å–å€¼å¹³å‡PCEçš„å·®è·
            improvement = value_mean - original_pce

            # è°ƒæ•´æ”¹è¿›å€¼ï¼Œé¿å…è¿‡å¤§æˆ–è¿‡å°
            if improvement < self.min_improvement:
                # å¦‚æœæ”¹è¿›å€¼å¤ªå°ï¼Œä½¿ç”¨ç›¸å¯¹æ€§èƒ½æ¥è°ƒæ•´
                relative_improvement = self.min_improvement * (1 + impact_info['relative_performance'] / 10)
                improvement = max(self.min_improvement, min(relative_improvement, self.max_improvement))
            elif improvement > self.max_improvement:
                improvement = self.max_improvement

            # ç¡®ä¿æ”¹è¿›å€¼åœ¨èŒƒå›´å†…
            improvement = max(self.min_improvement, min(improvement, self.max_improvement))

            # é¢„æµ‹PCE = åŸå§‹PCE + æ”¹è¿›å€¼
            predicted_pce = original_pce + improvement

            # æ·»åŠ ç½®ä¿¡åº¦ï¼ˆåŸºäºæ•°æ®é‡å’Œæ ‡å‡†å·®ï¼‰
            # æ•°æ®è¶Šå¤šï¼Œç½®ä¿¡åº¦è¶Šé«˜ï¼›æ ‡å‡†å·®è¶Šå°ï¼Œç½®ä¿¡åº¦è¶Šé«˜
            base_confidence = min(80, count * 3)  # æ¯1ä¸ªæ•°æ®ç‚¹å¢åŠ 3%ç½®ä¿¡åº¦ï¼Œæœ€é«˜80%

            # åŸºäºæ ‡å‡†å·®è°ƒæ•´ç½®ä¿¡åº¦
            if value_std > 0:
                std_factor = max(0, 1 - (value_std / overall_std) * 0.5)
                confidence = base_confidence * std_factor
            else:
                confidence = base_confidence

            confidence = min(100, max(20, confidence))  # ç¡®ä¿åœ¨20-100%ä¹‹é—´

            predictions.append({
                'record_id': record_id,
                'encoded_value': encoded_value,
                'original_value': impact_info['original_value'],
                'display_value': impact_info['display'],
                'predicted_pce': round(predicted_pce, 4),
                'original_pce': original_pce,
                'value_mean_pce': round(value_mean, 4),
                'improvement': round(improvement, 4),
                'value_pce_std': round(value_std, 4),
                'data_count': count,
                'confidence': round(confidence, 1),
                'relative_performance': round(impact_info['relative_performance'], 4),
                'method': 'value_mean_based' if improvement >= self.min_improvement else 'relative_performance_based'
            })

        return predictions

    def optimize_feature_for_record(self, record_idx, record):
        """ä¸ºå•æ¡è®°å½•ä¼˜åŒ–ç‰¹å¾"""
        record_id = record.get('Record', f'Record_{record_idx + 1}')
        original_pce = record.get('PCE', 0)

        print(f"\n{'=' * 60}")
        print(f"ğŸš€ ä¼˜åŒ–è®°å½• {record_idx + 1} (ID: {record_id}, PCE: {original_pce:.2f}%)")
        print(f"{'=' * 60}")

        record_results = {}

        # å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡Œä¼˜åŒ–
        for feature_idx, feature_name in enumerate(self.target_features, 1):
            print(f"\nğŸ” ä¼˜åŒ–ç‰¹å¾ {feature_idx}/3: {feature_name}")

            # è·å–åŸå§‹å€¼
            original_encoded = record.get(feature_name, '')
            original_decoded = self.decode_value(feature_name, original_encoded)
            print(f"  åŸå§‹å€¼: '{original_decoded}' (ç¼–ç : {original_encoded})")

            # åˆ†æè¯¥ç‰¹å¾çš„å½±å“
            feature_impact = self.analyze_feature_impact(feature_name)

            if not feature_impact:
                print(f"  âš ï¸ æ— æ³•åˆ†æè¯¥ç‰¹å¾çš„å½±å“")
                continue

            # æ˜¾ç¤ºåŸå§‹å€¼çš„è¡¨ç°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if original_encoded in feature_impact:
                original_impact = feature_impact[original_encoded]
                print(f"  åŸå§‹å€¼çš„å¹³å‡PCE: {original_impact['pce_mean']:.4f}% (åŸºäº{original_impact['count']}æ¡æ•°æ®)")
                print(f"  åŸå§‹å€¼ç›¸å¯¹æ€§èƒ½: {original_impact['relative_performance']:.4f}")

            # è®¡ç®—åŸºäºåç§»çš„é¢„æµ‹
            predictions = self.calculate_offset_predictions(
                original_pce, feature_impact, feature_name, original_encoded, record_id
            )

            if not predictions:
                print(f"  âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ›¿ä»£å–å€¼")
                continue

            # æŒ‰é¢„æµ‹PCEé™åºæ’åº
            predictions.sort(key=lambda x: x['predicted_pce'], reverse=True)

            # å–å‰3ä¸ª
            top_3 = predictions[:3]

            print(f"\n  ğŸ“Š å‰3ä¸ªæœ€ä½³å–å€¼:")
            for i, pred in enumerate(top_3):
                print(f"    {i + 1}. {pred['display_value']}")
                print(f"        é¢„æµ‹PCE: {pred['predicted_pce']:.4f}%, "
                      f"æ”¹è¿›: {pred['improvement']:+.4f}%")
                print(f"        è¯¥å–å€¼å¹³å‡PCE: {pred['value_mean_pce']:.4f}%, "
                      f"ç›¸å¯¹æ€§èƒ½: {pred['relative_performance']:+.4f}")
                print(f"        æ•°æ®é‡: {pred['data_count']}, ç½®ä¿¡åº¦: {pred['confidence']:.1f}%")
                print(f"        è®¡ç®—æ–¹æ³•: {pred['method']}")

            # å­˜å‚¨ç»“æœ
            record_results[feature_name] = {
                'original_encoded': original_encoded,
                'original_decoded': original_decoded,
                'original_pce': original_pce,
                'top_3_values': top_3
            }

        return {
            'record_id': record_id,
            'original_pce': original_pce,
            'optimization_results': record_results
        }

    def run_optimization(self):
        """è¿è¡Œä¼˜åŒ–è¿‡ç¨‹"""
        print("\n" + "=" * 60)
        print("ğŸ¯ åŸºäºåç§»çš„ç‰¹å¾ä¼˜åŒ–")
        print("=" * 60)
        print(f"ğŸ“Š æ–¹æ³•: åŸºäºç‰¹å¾å–å€¼åœ¨æ•°æ®åº“ä¸­çš„å¹³å‡PCEè¡¨ç°è®¡ç®—åç§»é‡")
        print(f"ğŸ“ˆ é¢„æµ‹PCE = åŸå§‹PCE + åŸºäºå–å€¼å¹³å‡PCEçš„æ”¹è¿›å€¼")
        print(f"ğŸ“‹ æ”¹è¿›å€¼èŒƒå›´: {self.min_improvement}% - {self.max_improvement}%")

        if self.bestnip_records is None or len(self.bestnip_records) == 0:
            print("âŒ BestNIPæ–‡ä»¶ä¸­æ²¡æœ‰è®°å½•")
            return

        # å­˜å‚¨æ‰€æœ‰ç»“æœ
        all_results = []

        # å¯¹æ¯æ¡è®°å½•è¿›è¡Œä¼˜åŒ–
        for record_idx, (_, record) in enumerate(self.bestnip_records.iterrows()):
            result = self.optimize_feature_for_record(record_idx, record)
            if result:
                all_results.append(result)

        # ä¿å­˜ç»“æœ
        self.save_results(all_results)

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(all_results)

        return all_results

    def save_results(self, all_results):
        """ä¿å­˜ä¼˜åŒ–ç»“æœ"""
        try:
            # åˆ›å»ºè¯¦ç»†ç»“æœDataFrame
            detailed_results = []

            for result_idx, result in enumerate(all_results):
                record_id = result['record_id']
                original_pce = result['original_pce']

                for feature_name, feature_result in result['optimization_results'].items():
                    # è·å–å‰3ä¸ªå€¼
                    for rank, top_value in enumerate(feature_result['top_3_values'], 1):
                        detailed_results.append({
                            'Record_Index': result_idx + 1,
                            'Record_ID': record_id,
                            'Original_PCE': original_pce,
                            'Feature': feature_name,
                            'Rank': rank,
                            'Original_Value_Encoded': feature_result['original_encoded'],
                            'Original_Value_Decoded': feature_result['original_decoded'],
                            'Recommended_Value_Encoded': top_value['encoded_value'],
                            'Recommended_Value_Decoded': top_value['original_value'],
                            'Display_Value': top_value['display_value'],
                            'Predicted_PCE': top_value['predicted_pce'],
                            'Improvement': top_value['improvement'],
                            'Value_Mean_PCE': top_value['value_mean_pce'],
                            'Value_PCE_Std': top_value['value_pce_std'],
                            'Relative_Performance': top_value['relative_performance'],
                            'Data_Count': top_value['data_count'],
                            'Confidence': top_value['confidence'],
                            'Calculation_Method': top_value['method']
                        })

            # è½¬æ¢ä¸ºDataFrame
            results_df = pd.DataFrame(detailed_results)

            # ä¿å­˜åˆ°Excel
            filename = f"{self.results_dir}/offset_optimization_top3_{self.timestamp}.xlsx"
            results_df.to_excel(filename, index=False)
            print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {filename}")

            # åŒæ—¶ä¿å­˜ä¸ºCSVæ ¼å¼
            csv_filename = f"{self.results_dir}/offset_optimization_top3_{self.timestamp}.csv"
            results_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜ä¸ºCSV: {csv_filename}")

            # ä¿å­˜æ±‡æ€»ç»“æœ
            self.save_summary_results(all_results)

        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

    def save_summary_results(self, all_results):
        """ä¿å­˜æ±‡æ€»ç»“æœ"""
        try:
            # åˆ›å»ºæ±‡æ€»è¡¨æ ¼
            summary_data = []

            for result_idx, result in enumerate(all_results):
                record_id = result['record_id']
                original_pce = result['original_pce']

                summary_row = {
                    'Record_Index': result_idx + 1,
                    'Record_ID': record_id,
                    'Original_PCE': original_pce
                }

                # æ·»åŠ æ¯ä¸ªç‰¹å¾çš„æœ€ä½³æ¨èï¼ˆç¬¬1åï¼‰
                for feature_name in self.target_features:
                    if feature_name in result['optimization_results']:
                        top_values = result['optimization_results'][feature_name]['top_3_values']
                        if top_values:
                            best_value = top_values[0]
                            summary_row[f'{feature_name}_Original'] = result['optimization_results'][feature_name][
                                'original_decoded']
                            summary_row[f'{feature_name}_Best_Value'] = best_value['original_value']
                            summary_row[f'{feature_name}_Display'] = best_value['display_value']
                            summary_row[f'{feature_name}_Predicted_PCE'] = best_value['predicted_pce']
                            summary_row[f'{feature_name}_Improvement'] = best_value['improvement']
                            summary_row[f'{feature_name}_Confidence'] = best_value['confidence']
                            summary_row[f'{feature_name}_Method'] = best_value['method']

                summary_data.append(summary_row)

            # åˆ›å»ºæ±‡æ€»DataFrame
            summary_df = pd.DataFrame(summary_data)

            # ä¿å­˜æ±‡æ€»ç»“æœ
            summary_filename = f"{self.results_dir}/offset_summary_{self.timestamp}.xlsx"
            summary_df.to_excel(summary_filename, index=False)
            print(f"ğŸ’¾ æ±‡æ€»ç»“æœå·²ä¿å­˜: {summary_filename}")

        except Exception as e:
            print(f"âŒ ä¿å­˜æ±‡æ€»ç»“æœå¤±è´¥: {e}")

    def generate_report(self, all_results):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        try:
            # è®¡ç®—æ€»ä½“ç»Ÿè®¡
            overall_mean_pce = self.df['PCE'].mean()

            report_content = []
            report_content.append("åŸºäºåç§»çš„ç‰¹å¾ä¼˜åŒ–æŠ¥å‘Š")
            report_content.append("=" * 70)
            report_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report_content.append(f"æ•°æ®åº“æ–‡ä»¶: {self.data_path}")
            report_content.append(f"ç›®æ ‡è®°å½•æ–‡ä»¶: {self.bestnip_path}")
            report_content.append(f"ä¼˜åŒ–ç‰¹å¾: {', '.join(self.target_features)}")
            report_content.append(f"æ”¹è¿›å€¼èŒƒå›´: {self.min_improvement}% - {self.max_improvement}%")
            report_content.append(f"æ•°æ®åº“æ€»ä½“å¹³å‡PCE: {overall_mean_pce:.4f}%")
            report_content.append("")

            report_content.append("ğŸ“Š ä¼˜åŒ–æ–¹æ³•è¯´æ˜:")
            report_content.append("1. åˆ†ææ•°æ®åº“ä¸­æ¯ä¸ªç‰¹å¾å–å€¼çš„å¹³å‡PCEè¡¨ç°")
            report_content.append("2. å¯¹äºæ¯æ¡è®°å½•ï¼ŒåŸºäºåŸå§‹PCEå’Œç‰¹å¾å–å€¼çš„å¹³å‡PCEè®¡ç®—æ”¹è¿›å€¼")
            report_content.append("3. é¢„æµ‹PCE = åŸå§‹PCE + åŸºäºå–å€¼å¹³å‡PCEçš„æ”¹è¿›å€¼")
            report_content.append("4. ç½®ä¿¡åº¦åŸºäºæ•°æ®é‡å’Œæ ‡å‡†å·®è®¡ç®—")
            report_content.append("5. é€‰æ‹©é¢„æµ‹PCEæœ€é«˜çš„å‰3ä¸ªå–å€¼")
            report_content.append("")

            report_content.append("ğŸ“Š ä¼˜åŒ–ç»“æœ:")
            report_content.append("-" * 70)

            for result_idx, result in enumerate(all_results):
                record_id = result['record_id']
                original_pce = result['original_pce']

                report_content.append(f"\nè®°å½• {result_idx + 1} (ID: {record_id}):")
                report_content.append(f"  åŸå§‹PCE: {original_pce:.2f}%")

                for feature_name in self.target_features:
                    if feature_name in result['optimization_results']:
                        feature_result = result['optimization_results'][feature_name]

                        report_content.append(f"\n  {feature_name}:")
                        report_content.append(f"    åŸå§‹å€¼: {feature_result['original_decoded']}")

                        for i, top_value in enumerate(feature_result['top_3_values'], 1):
                            report_content.append(f"    ç¬¬{i}å: {top_value['display_value']}")
                            report_content.append(f"        é¢„æµ‹PCE: {top_value['predicted_pce']:.4f}%, "
                                                  f"æ”¹è¿›: {top_value['improvement']:+.4f}%")
                            report_content.append(f"        è¯¥å–å€¼å¹³å‡PCE: {top_value['value_mean_pce']:.4f}%, "
                                                  f"ç›¸å¯¹æ€§èƒ½: {top_value['relative_performance']:+.4f}")
                            report_content.append(f"        ç½®ä¿¡åº¦: {top_value['confidence']:.1f}%, "
                                                  f"æ•°æ®é‡: {top_value['data_count']}æ¡")
                            report_content.append(f"        è®¡ç®—æ–¹æ³•: {top_value['method']}")

                report_content.append("-" * 40)

            # ä¿å­˜æŠ¥å‘Š
            report_filename = f"{self.results_dir}/offset_optimization_report_{self.timestamp}.txt"
            with open(report_filename, 'w', encoding='utf-8') as f:
                f.write('\n'.join(report_content))

            print(f"ğŸ“‹ ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜: {report_filename}")

            # æ‰“å°æŠ¥å‘Šæ‘˜è¦
            print("\n" + "=" * 60)
            print("ğŸ“Š ä¼˜åŒ–å®Œæˆ!")
            print("=" * 60)
            print(f"âœ… å·²ä¼˜åŒ– {len(all_results)} æ¡è®°å½•")
            print(f"âœ… æ¯ä¸ªç‰¹å¾æ‰¾åˆ°å‰3ä¸ªæœ€ä½³å–å€¼")
            print(f"âœ… åŸºäºå–å€¼å¹³å‡PCEè®¡ç®—æ–¹æ³•ï¼Œæ”¹è¿›å€¼é™åˆ¶åœ¨{self.min_improvement}-{self.max_improvement}ä¹‹é—´")
            print(f"âœ… ç»“æœä¿å­˜åœ¨: {self.results_dir}/ ç›®å½•ä¸‹")

            # æ˜¾ç¤ºæœ€ä½³æ”¹è¿›
            print(f"\nğŸ† æœ€ä½³æ”¹è¿›æ€»ç»“:")

            best_improvements = []
            for result_idx, result in enumerate(all_results):
                for feature_name in self.target_features:
                    if feature_name in result['optimization_results']:
                        top_values = result['optimization_results'][feature_name]['top_3_values']
                        if top_values:
                            best_value = top_values[0]
                            best_improvements.append({
                                'Record': result_idx + 1,
                                'Feature': feature_name,
                                'Best_Value': best_value['original_value'],
                                'Display': best_value['display_value'],
                                'Predicted_PCE': best_value['predicted_pce'],
                                'Improvement': best_value['improvement'],
                                'Confidence': best_value['confidence'],
                                'Method': best_value['method']
                            })

            # æŒ‰é¢„æµ‹PCEæ’åº
            best_improvements.sort(key=lambda x: x['Predicted_PCE'], reverse=True)

            for i, imp in enumerate(best_improvements[:5]):
                print(f"{i + 1}. è®°å½•{imp['Record']}çš„{imp['Feature']}: {imp['Display']}")
                print(f"   é¢„æµ‹PCE: {imp['Predicted_PCE']:.4f}%, "
                      f"æ”¹è¿›: {imp['Improvement']:+.4f}%, "
                      f"ç½®ä¿¡åº¦: {imp['Confidence']:.1f}%, "
                      f"æ–¹æ³•: {imp['Method']}")

        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” åŸºäºåç§»çš„ç‰¹å¾ä¼˜åŒ–ç³»ç»Ÿ")
    print("ğŸ¯ ç›®æ ‡: ä¼˜åŒ–bestnip.xlsxä¸­è®°å½•çš„ETL_Passivatorã€HTL_Passivatorå’ŒPrecursor_Solution_Addictive")
    print("ğŸ“Š è¾“å‡º: æ¯ä¸ªç‰¹å¾æ›¿æ¢åPCEæœ€é«˜çš„å‰3ä¸ªå–å€¼")
    print(f"ğŸ”„ æ–¹æ³•: åŸºäºç‰¹å¾å–å€¼åœ¨æ•°æ®åº“ä¸­çš„å¹³å‡PCEè¡¨ç°è®¡ç®—æ”¹è¿›å€¼ï¼Œæ”¹è¿›å€¼é™åˆ¶åœ¨0.1-1.0ä¹‹é—´")
    print(f"ğŸ“ æ•°æ®é›†: FinalData.xlsx")

    try:
        # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹
        optimizer = OffsetBasedOptimizer(
            data_path="FinalData.xlsx",
            bestnip_path="bestnip.xlsx",
            min_improvement=0.1,  # æœ€å°æ”¹è¿›å€¼0.1%
            max_improvement=1.0  # æœ€å¤§æ”¹è¿›å€¼1.0%
        )

        # è¿è¡Œä¼˜åŒ–
        results = optimizer.run_optimization()

    except FileNotFoundError as e:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨:")
        print("   - FinalData.xlsx (æ•°æ®åº“æ–‡ä»¶)")
        print("   - bestnip.xlsx (ç›®æ ‡è®°å½•æ–‡ä»¶)")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()