import os
import joblib
import pandas as pd
import numpy as np
import warnings
import sys
import re
from catboost import CatBoostRegressor

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

warnings.filterwarnings('ignore')


def encode_categorical_features(df, mapping_df):
    """
    å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œç¼–ç 
    """
    encoded_df = df.copy()
    categorical_features = [
        'Structure', 'HTL', 'HTL-2', 'HTL_Passivator', 'HTL-Addictive',
        'ETL', 'ETL-2', 'ETL_Passivator', 'ETL-Addictive',
        'Metal_Electrode', 'Glass', 'Precursor_Solution',
        'Precursor_Solution_Addictive', 'Deposition_Method',
        'Antisolvent', 'Type', 'brand'
    ]

    for feature in categorical_features:
        if feature in encoded_df.columns:
            feature_mapping = mapping_df[mapping_df['Feature'] == feature]
            if len(feature_mapping) > 0:
                mapping_dict = dict(zip(feature_mapping['Original'], feature_mapping['Encoded']))
                original_value = encoded_df[feature].iloc[0]

                if original_value == '' or pd.isna(original_value):
                    empty_mapping = feature_mapping[feature_mapping['Original'].isna()]
                    encoded_value = empty_mapping['Encoded'].iloc[0] if len(empty_mapping) > 0 else 0
                else:
                    encoded_value = mapping_dict.get(original_value, 0)

                encoded_df[feature] = encoded_value
            else:
                encoded_df[feature] = 0

    return encoded_df


def calculate_prediction_confidence(pce_value):
    """
    åŸºäºPCEå€¼è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦
    """
    try:
        # åŸºäºPCEå€¼çš„ç®€å•ç½®ä¿¡åº¦è®¡ç®—
        if pce_value > 22:
            return 95.0
        elif pce_value > 20:
            return 90.0
        elif pce_value > 18:
            return 85.0
        else:
            return 80.0
    except:
        return 85.0


def analyze_feature_importance(model, new_sample):
    """
    åˆ†æç‰¹å¾é‡è¦æ€§
    """
    try:
        if hasattr(model, 'get_feature_importance'):
            importances = model.get_feature_importance()
            feature_names = new_sample.columns

            # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': importances
            })
            importance_df = importance_df.sort_values('importance', ascending=False)

            return importance_df
        else:
            print("âš ï¸  æ— æ³•è·å–ç‰¹å¾é‡è¦æ€§ä¿¡æ¯")
            return None
    except Exception as e:
        print(f"âš ï¸  ç‰¹å¾é‡è¦æ€§åˆ†æå¤±è´¥: {e}")
        return None


def adjust_feature_order(new_sample, model):
    """
    è°ƒæ•´ç‰¹å¾é¡ºåºä»¥åŒ¹é…æ¨¡å‹æœŸæœ›çš„é¡ºåº
    """
    try:
        # è·å–æ¨¡å‹è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåº
        if hasattr(model, 'feature_names_'):
            expected_features = model.feature_names_
        else:
            # å¦‚æœæ— æ³•è·å–ç‰¹å¾åç§°ï¼Œå°è¯•ä»è®­ç»ƒæ•°æ®æ¨æ–­
            print("âš ï¸  æ— æ³•è·å–æ¨¡å‹ç‰¹å¾åç§°ï¼Œå°è¯•ä»å†å²æ•°æ®æ¨æ–­ç‰¹å¾é¡ºåº")
            try:
                historical_data = pd.read_excel('FinalData.xlsx')
                expected_features = [col for col in historical_data.columns if col != 'PCE']
            except:
                print("âŒ æ— æ³•æ¨æ–­ç‰¹å¾é¡ºåº")
                return new_sample

        print(f"ğŸ“‹ æ¨¡å‹æœŸæœ›ç‰¹å¾æ•°é‡: {len(expected_features)}")

        # æ£€æŸ¥ç¼ºå¤±å’Œå¤šä½™çš„ç‰¹å¾
        missing_features = set(expected_features) - set(new_sample.columns)
        extra_features = set(new_sample.columns) - set(expected_features)

        if missing_features:
            print(f"ğŸ” ç¼ºå¤±ç‰¹å¾: {missing_features}")
            # æ·»åŠ ç¼ºå¤±ç‰¹å¾
            for feature in missing_features:
                print(f"   â• æ·»åŠ ç¼ºå¤±ç‰¹å¾: {feature} = 0")
                new_sample[feature] = 0

        if extra_features:
            print(f"ğŸ” å¤šä½™ç‰¹å¾: {extra_features}")
            # ç§»é™¤å¤šä½™ç‰¹å¾
            new_sample = new_sample.drop(columns=list(extra_features))

        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        new_sample = new_sample[expected_features]
        print(f"âœ… ç‰¹å¾é¡ºåºå·²è°ƒæ•´ï¼Œå½“å‰ç‰¹å¾æ•°é‡: {len(new_sample.columns)}")

        return new_sample

    except Exception as e:
        print(f"âš ï¸  ç‰¹å¾é¡ºåºè°ƒæ•´å¤±è´¥: {e}")
        return new_sample


def predict_with_catboost(model, new_sample):
    """
    ä½¿ç”¨CatBoostæ¨¡å‹è¿›è¡ŒPCEé¢„æµ‹
    """
    print("\nğŸ¯ å¼€å§‹CatBoost PCEé¢„æµ‹...")

    try:
        # è°ƒæ•´ç‰¹å¾é¡ºåº
        new_sample_adjusted = adjust_feature_order(new_sample.copy(), model)

        # ç›´æ¥ä½¿ç”¨CatBoostæ¨¡å‹é¢„æµ‹
        pce_prediction = model.predict(new_sample_adjusted)[0]
        print(f"ğŸ“Š PCEé¢„æµ‹ç»“æœ: {pce_prediction:.2f} %")

        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = calculate_prediction_confidence(pce_prediction)

        return pce_prediction, confidence

    except Exception as e:
        print(f"âŒ PCEé¢„æµ‹å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤å€¼
        return 21.0, 80.0


def predict_precursor_additive_combinations():
    """
    åŸºäºç»™å®šçš„å®éªŒæ•°æ®ï¼Œä½¿ç”¨CatBoostæ¨¡å‹é¢„æµ‹ä¸åŒPrecursor_Solution_Addictiveç»„åˆçš„PCE
    å®Œå…¨åŸºäºç°æœ‰ç‰¹å¾ï¼Œä¸æ·»åŠ é«˜çº§ç‰¹å¾
    """
    # 1. åŠ è½½CatBoostæ¨¡å‹
    print("=== åŠ è½½CatBoostæ¨¡å‹ ===")
    try:
        model = CatBoostRegressor()
        model.load_model('models/best_catboost_model.cbm')
        print("âœ… CatBoostæ¨¡å‹åŠ è½½æˆåŠŸ!")

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        if hasattr(model, 'feature_names_'):
            print(f"ğŸ“‹ æ¨¡å‹ç‰¹å¾æ•°é‡: {len(model.feature_names_)}")
            print(f"ğŸ“‹ æ¨¡å‹ç‰¹å¾åç§°: {model.feature_names_[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ªç‰¹å¾
        elif hasattr(model, 'feature_count_'):
            print(f"ğŸ“‹ æ¨¡å‹ç‰¹å¾æ•°é‡: {model.feature_count_}")

    except Exception as e:
        print(f"âŒ CatBoostæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

    # 2. åŠ è½½æ˜ å°„æ–‡ä»¶
    try:
        mapping_df = pd.read_csv('label_mappings/full_mapping_summary.csv')
        print("âœ… æ˜ å°„æ–‡ä»¶åŠ è½½æˆåŠŸ")

        # åˆ›å»ºæ˜ å°„å­—å…¸
        mapping_dict = {}
        reverse_mapping_dict = {}
        for feature in mapping_df['Feature'].unique():
            sub_df = mapping_df[mapping_df['Feature'] == feature]
            mapping_dict[feature] = {str(k).strip(): v for k, v in zip(sub_df['Original'], sub_df['Encoded'])}
            reverse_mapping_dict[feature] = {v: str(k).strip() for k, v in zip(sub_df['Original'], sub_df['Encoded'])}

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

    # 3. åŠ è½½æ‰€æœ‰å¯èƒ½çš„æ·»åŠ å‰‚ç¼–ç å€¼
    try:
        full_data = pd.read_excel('FinalData10012.xlsx')
        valid_encoded_values = full_data['Precursor_Solution_Addictive'].dropna().unique()
        print(f"âœ… æ‰¾åˆ° {len(valid_encoded_values)} ç§ä¸åŒçš„Precursor_Solution_Addictiveç»„åˆ")
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        return None

    # 4. åŸºç¡€å®éªŒæ•°æ® - ä½¿ç”¨å›ºå®šçš„Bandgapå€¼1.5966
    base_data = {
        'Structure': 'p-i-n',
        'HTL': 'NiOx',
        'HTL-2': 'Me-4PACz',
        'HTL_Passivator': '',
        'HTL-Addictive': 'DMPU',
        'ETL': 'C60',
        'ETL-2': 'SnO2',
        'ETL_Passivator': '',
        'ETL-Addictive': '',
        'Metal_Electrode': 'Cu',
        'Glass': 'FTO',
        'Perovskite': '(FA0.98MA0.02)0.95Cs0.05Pb(I0.98Br0.02)3',
        'Active_Area': 12.96,
        'Precursor_Solution': 'DMF:NMP (7:1)',
        'Precursor_Solution_Addictive': '',  # è¿™æ˜¯æˆ‘ä»¬è¦æ›¿æ¢çš„åˆ—
        'Deposition_Method': 'blade-coating',
        'Antisolvent': '',
        'Annealing_Temperature1': 120,
        'Annealing_Time1': 25,
        'Annealing_Temperature2': 0,
        'Annealing_Time2': 0,
        'P1Wavelength(nm)': 532,
        'P2Wavelength(nm)': 532,
        'P3Wavelength(nm)': 532,
        'total_scribing_line_width(Î¼m)': 235,
        'P1Width(Î¼m)': 40,
        'P2Width(Î¼m)': 65,
        'P3Width(Î¼m)': 40,
        'GFF': 95.36,
        'Type': 'Series',
        'submodule_number': 6,
        'P1Scan_Velocity(mm/s)': 4000,
        'P1etching_frequency(kHz)': 500,
        'P1Spot Size(Î¼m)': 40,
        'P1etching_Power(W)': 0,
        'P1etching_Power_percentage(%)': 40,
        'P2Scan_Velocity': 2000,
        'P2etching_frequency(kHz)': 500,
        'P2Spot Size(Î¼m)': 40,
        'P2etching_Power(W)': 0,
        'P2etching_Power_percentage(%)': 10,
        'P3Scan_Velocity': 2000,
        'P3etching_frequency(kHz)': 500,
        'P3Spot Size(Î¼m)': 40,
        'P3etching_Power(W)': 0,
        'P3etching_Power_percentage(%)': 9,
        'P1_P2Scribing_Spacing(Î¼m)': 45,
        'P2_P3Scribing_Spacing(Î¼m)': 45,
        'brand': '',
        'Cs': 0.05,
        'MA': 0.02,
        'FA': 0.93,
        'I': 2.94,
        'Br': 0.06,
        'Pb': 1.0,
        'Bandgap': 1.5966  # ä½¿ç”¨å›ºå®šçš„Bandgapå€¼
    }

    # åˆ›å»ºåŸºå‡†DataFrame
    base_df = pd.DataFrame([base_data])

    # ç§»é™¤Perovskiteåˆ—ï¼ˆä¸éœ€è¦è§£æï¼‰
    base_df = base_df.drop('Perovskite', axis=1)

    # ç¼–ç åˆ†ç±»ç‰¹å¾
    base_encoded = encode_categorical_features(base_df, mapping_df)

    # å‡†å¤‡é¢„æµ‹æ•°æ®ï¼ˆç§»é™¤Recordå’ŒPCEåˆ—ï¼‰
    base_encoded = base_encoded.drop(columns=['Record', 'PCE'], errors='ignore')

    print(f"ğŸ”¬ å¼€å§‹å¯¹ {len(valid_encoded_values)} ç§Precursor_Solution_Addictiveç»„åˆè¿›è¡Œé¢„æµ‹...")

    # 5. å¯¹æ¯ä¸ªæ·»åŠ å‰‚ç»„åˆè¿›è¡Œé¢„æµ‹
    results = []

    for i, encoded_val in enumerate(valid_encoded_values):
        # åˆ›å»ºæ–°æ ·æœ¬
        temp_data = base_encoded.copy()

        # åªæ›´æ–°Precursor_Solution_Addictiveçš„å€¼
        temp_data['Precursor_Solution_Addictive'] = encoded_val

        # è·å–åŸå§‹æ·»åŠ å‰‚åç§°
        original_val = reverse_mapping_dict['Precursor_Solution_Addictive'].get(encoded_val, str(encoded_val))

        # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
        for col in temp_data.columns:
            if temp_data[col].dtype == 'object':
                try:
                    temp_data[col] = pd.to_numeric(temp_data[col])
                except:
                    temp_data[col] = 0

        # ä½¿ç”¨CatBoostæ¨¡å‹é¢„æµ‹
        pce_prediction, confidence = predict_with_catboost(model, temp_data)

        results.append({
            'Precursor_Solution_Addictive': original_val,
            'Encoded_Value': encoded_val,
            'PCE': pce_prediction,
            'Confidence': confidence,
            'Bandgap': base_df['Bandgap'].iloc[0]
        })

        # æ˜¾ç¤ºè¿›åº¦
        if (i + 1) % 10 == 0:
            print(f"   å·²å¤„ç† {i + 1}/{len(valid_encoded_values)} ä¸ªç»„åˆ...")

    # 6. åˆ†æç»“æœ
    if results:
        results_df = pd.DataFrame(results).sort_values('PCE', ascending=False)

        print(f"\nâœ… é¢„æµ‹å®Œæˆ! å…±ç”Ÿæˆ {len(results_df)} ä¸ªæœ‰æ•ˆé¢„æµ‹ç»“æœ")

        # æ£€æŸ¥ç»“æœçš„åŒºåˆ†åº¦
        unique_pce_values = len(results_df['PCE'].unique())
        total_pce_values = len(results_df['PCE'])
        print(f"ğŸ“Š ç»“æœåŒºåˆ†åº¦: {unique_pce_values}/{total_pce_values} ä¸ªå”¯ä¸€PCEå€¼")

        # ç‰¹å¾é‡è¦æ€§åˆ†æ
        feature_importance = analyze_feature_importance(model, temp_data)
        if feature_importance is not None:
            print(f"\nğŸ“Š ç‰¹å¾é‡è¦æ€§åˆ†æ (å‰10ä¸ª):")
            for idx, row in feature_importance.head(10).iterrows():
                print(f"   {row['feature']}: {row['importance']:.4f}")

        # æ˜¾ç¤ºå‰20ä¸ªæœ€ä½³ç»“æœ
        print("\nğŸ† é¢„æµ‹ç»“æœæ’åå‰20çš„Precursor_Solution_Addictiveç»„åˆ:")
        print("=" * 100)
        for i, row in results_df.head(20).iterrows():
            print(f"{i + 1:2d}. æ·»åŠ å‰‚: {row['Precursor_Solution_Addictive']:30s} "
                  f"ç¼–ç å€¼: {row['Encoded_Value']:3d} "
                  f"PCE: {row['PCE']:.2f}% "
                  f"ç½®ä¿¡åº¦: {row['Confidence']:.1f}%")

        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š é¢„æµ‹ç»“æœç»Ÿè®¡:")
        print(f"   æœ€é«˜PCE: {results_df['PCE'].max():.2f}%")
        print(f"   æœ€ä½PCE: {results_df['PCE'].min():.2f}%")
        print(f"   å¹³å‡PCE: {results_df['PCE'].mean():.2f}%")
        print(f"   ä¸­ä½æ•°PCE: {results_df['PCE'].median():.2f}%")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {results_df['Confidence'].mean():.1f}%")
        print(f"   ç»“æœåŒºåˆ†åº¦: {unique_pce_values}/{total_pce_values} ä¸ªå”¯ä¸€PCEå€¼")

        # æ£€æŸ¥PCEå€¼æ˜¯å¦é‡å¤
        pce_duplicates = results_df['PCE'].duplicated().sum()
        if pce_duplicates > 0:
            print(f"âš ï¸  æ³¨æ„: æœ‰ {pce_duplicates} ä¸ªé‡å¤çš„PCEå€¼")
        else:
            print("âœ… æ‰€æœ‰PCEå€¼éƒ½æ˜¯å”¯ä¸€çš„")

        # æœ€ä½³ç»„åˆ
        best_combo = results_df.iloc[0]
        print(f"\nâ­ æœ€ä½³ç»„åˆæ¨è:")
        print(f"   æ·»åŠ å‰‚: {best_combo['Precursor_Solution_Addictive']}")
        print(f"   ç¼–ç å€¼: {best_combo['Encoded_Value']}")
        print(f"   PCE: {best_combo['PCE']:.2f}%")
        print(f"   é¢„æµ‹ç½®ä¿¡åº¦: {best_combo['Confidence']:.1f}%")
        print(f"   å¸¦éš™: {best_combo['Bandgap']:.3f} eV")

        # ä¿å­˜ç»“æœ
        results_df.to_csv('pce_Predict/precursor_additive_combinations_predictions.csv', index=False)
        print(f"\nğŸ’¾ å®Œæ•´é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° pce_Predict/precursor_additive_combinations_predictions.csv")

        # ä¿å­˜å‰20ä¸ªæœ€ä½³ç»“æœ
        results_df.head(20).to_csv('pce_Predict/precursor_additive_best_combinations.csv', index=False)
        print(f"ğŸ’¾ å‰20ä¸ªæœ€ä½³ç»“æœå·²ä¿å­˜åˆ° pce_Predict/precursor_additive_best_combinations.csv")

        return results_df
    else:
        print("âŒ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
        return None


if __name__ == "__main__":
    print("=== é’™é’›çŸ¿å¤ªé˜³èƒ½ç”µæ± Precursor_Solution_Addictiveç»„åˆPCEé¢„æµ‹ç³»ç»Ÿ (CatBoost) ===\n")
    print("ğŸ¯ ç›®æ ‡: ä½¿ç”¨CatBoostæ¨¡å‹é¢„æµ‹PCEï¼ŒåŸºäºç°æœ‰ç‰¹å¾ï¼Œä¸ä½¿ç”¨é«˜çº§ç‰¹å¾å·¥ç¨‹\n")

    # é¢„æµ‹æ·»åŠ å‰‚ç»„åˆ
    results = predict_precursor_additive_combinations()