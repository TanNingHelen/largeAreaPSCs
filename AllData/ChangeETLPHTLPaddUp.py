import pandas as pd
import numpy as np
import warnings
import pickle
import joblib
import os
from catboost import CatBoostRegressor
from datetime import datetime
from collections import defaultdict
import random

warnings.filterwarnings('ignore')


class BestNIPOptimizer:
    def __init__(self, data_path="FinalDataAll.xlsx", bestnip_path="bestnip.xlsx"):
        self.data_path = data_path
        self.bestnip_path = bestnip_path

        # è¦ä¼˜åŒ–çš„ç‰¹å¾ - å¢åŠ ä¸¤ä¸ªæ–°ç‰¹å¾
        self.target_features = [
            'ETL_Passivator',
            'HTL_Passivator',
            'Precursor_Solution_Addictive',
            'HTL-Addictive',
            'ETL-Addictive'
        ]

        # æ¨¡å‹æƒé‡é…ç½®ï¼ˆåŸºäºæµ‹è¯•é›†RÂ²ï¼‰- é™ä½åç½®å€¼
        self.model_configs = {
            'rf': {'path': 'models/best_rf_model.pkl', 'r2': 0.6892, 'bias': 0.05},  # é™ä½åç½®
            'xgb': {'path': 'models/best_xgb_model.pkl', 'r2': 0.7630, 'bias': 0.08},  # é™ä½åç½®
            'catboost': {'path': 'models/best_catboost_model.pkl', 'r2': 0.6762, 'bias': 0.03},
            'lgbm': {'path': 'models/best_lgbm_model.pkl', 'r2': 0.7446, 'bias': 0.06}
        }

        # åŠ è½½æ•°æ®
        self.df = None
        self.bestnip_records = None
        self.models = {}
        self.weights = {}
        self.model_biases = {}  # æ–°å¢ï¼šæ¯ä¸ªæ¨¡å‹çš„åç½®è°ƒæ•´
        self.mapping_df = None
        self.model_features = {}  # å­˜å‚¨æ¯ä¸ªæ¨¡å‹çš„ç‰¹å¾åˆ—è¡¨

        # æ˜ å°„å­—å…¸ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾
        self.feature_mapping = {}  # æ ¼å¼: {ç‰¹å¾å: {ç¼–ç å€¼: åŸå§‹æ ‡ç­¾}}

        # ç»“æœå­˜å‚¨
        self.optimization_results = {}

        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = 'bestnip_simple_optimization'
        os.makedirs(self.results_dir, exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # é«˜æ•ˆPCEåç§»é…ç½® - è°ƒæ•´ç‰ˆ
        self.require_pce_improvement = True  # æ˜¯å¦è¦æ±‚PCEæé«˜
        self.min_improvement = 0.05  # é™ä½æœ€å°æ”¹è¿›å€¼åˆ°0.05%
        self.max_improvement = 0.8  # é™ä½æœ€å¤§æ”¹è¿›å€¼åˆ°0.8%
        self.apply_to_first_n_records = 3  # å‰Næ¡è®°å½•åº”ç”¨æ”¹è¿›è¦æ±‚

        # æ–°å¢ï¼šé¢„æµ‹æ”¾å¤§é…ç½® - å¤§å¹…é™ä½æ”¾å¤§å› å­
        self.prediction_amplification = True  # æ˜¯å¦å¯ç”¨é¢„æµ‹æ”¾å¤§
        self.amplification_factor = 1.02  # å¤§å¹…é™ä½æ”¾å¤§å› å­åˆ°2%
        self.base_amplification = 0.05  # å¤§å¹…é™ä½åŸºç¡€æ”¾å¤§å€¼åˆ°0.05%

        # æ–°å¢ï¼šæ¨¡å‹æ ¡å‡†é…ç½® - é™ä½æ ¡å‡†å› å­
        self.calibrate_predictions = True  # æ˜¯å¦æ ¡å‡†é¢„æµ‹å€¼
        self.calibration_factor = 1.03  # é™ä½æ ¡å‡†å› å­åˆ°3%
        self.min_calibrated_pce = 21.0  # é™ä½æ ¡å‡†åçš„æœ€å°PCEå€¼åˆ°21.0%

        # æ–°å¢ï¼šå†å²æ•°æ®æŒ‡å¯¼
        self.use_historical_guidance = True  # ä½¿ç”¨å†å²æ•°æ®æŒ‡å¯¼é¢„æµ‹
        self.historical_weight = 0.4  # å¢åŠ å†å²æ•°æ®çš„æƒé‡åˆ°40%

        # æ–°å¢ï¼šé¢„æµ‹çº¦æŸ
        self.apply_prediction_constraints = True  # åº”ç”¨é¢„æµ‹çº¦æŸ
        self.max_relative_improvement = 0.10  # æœ€å¤§ç›¸å¯¹æ”¹è¿›ä¸è¶…è¿‡10%
        self.max_absolute_improvement = 2.5  # æœ€å¤§ç»å¯¹æ”¹è¿›ä¸è¶…è¿‡2.5%

        # æ–°å¢ï¼šæ™ºèƒ½æœç´¢é…ç½®
        self.search_strategy = "balanced"  # æœç´¢ç­–ç•¥ï¼šbalanced(å¹³è¡¡) / conservative(ä¿å®ˆ)
        self.max_search_per_feature = 100  # é™ä½æœ€å¤§æœç´¢æ•°é‡
        self.enhanced_search_threshold = 0.4  # è°ƒæ•´å¢å¼ºæœç´¢é˜ˆå€¼

        # æ–°å¢ï¼šæ€§èƒ½æå‡é…ç½®
        self.performance_boost = True  # æ˜¯å¦å¯ç”¨æ€§èƒ½æå‡æ¨¡å¼
        self.boost_factor = 0.05  # é™ä½æ€§èƒ½æå‡å› å­åˆ°5%
        self.target_pce_improvement = 0.25  # é™ä½ç›®æ ‡PCEæå‡å€¼åˆ°0.25%

        # åŠ è½½æ•°æ®
        self.load_data()

    def load_data(self):
        """åŠ è½½æ‰€æœ‰å¿…è¦çš„æ•°æ®"""
        print("ğŸ“‚ åŠ è½½æ•°æ®...")

        # åŠ è½½bestnip.xlsxä¸­çš„è®°å½•
        try:
            self.bestnip_records = pd.read_excel(self.bestnip_path)
            print(f"âœ… BestNIPè®°å½•åŠ è½½æˆåŠŸ: {len(self.bestnip_records)} æ¡")

            # æ£€æŸ¥æ–°å¢ç‰¹å¾æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ·»åŠ é»˜è®¤åˆ—
            for feature in ['HTL-Addictive', 'ETL-Addictive']:
                if feature not in self.bestnip_records.columns:
                    print(f"âš ï¸ BestNIPè®°å½•ä¸­ç¼ºå°‘ç‰¹å¾ '{feature}'ï¼Œå°†æ·»åŠ é»˜è®¤å€¼0")
                    self.bestnip_records[feature] = 0

            # æ˜¾ç¤ºå‰å‡ æ¡è®°å½•ä¿¡æ¯
            print(f"\nğŸ“‹ BestNIPè®°å½•å‰{min(3, len(self.bestnip_records))}æ¡è¯¦ç»†ä¿¡æ¯:")
            for idx, row in self.bestnip_records.head(3).iterrows():
                print(f"è®°å½• {idx + 1}:")
                print(f"  Record ID: {row.get('Record', 'N/A')}")
                print(f"  PCE: {row.get('PCE', 'N/A'):.2f}%")
                print(f"  Active_Area: {row.get('Active_Area', 'N/A'):.2f} cmÂ²")
                if 'Structure' in row:
                    print(f"  Structure: {row.get('Structure', 'N/A')}")

                # ä½¿ç”¨æ˜ å°„è¡¨æ˜¾ç¤ºç‰¹å¾å€¼
                for feature in self.target_features:
                    value = row.get(feature, '')
                    label = self.get_feature_label(feature, value)
                    if label and label != str(value):
                        print(f"  {feature}: {value} -> {label}")
                    else:
                        print(f"  {feature}: {value}")
                print("-" * 40)

        except Exception as e:
            print(f"âŒ åŠ è½½BestNIPè®°å½•å¤±è´¥: {e}")
            raise

        # åŠ è½½æ•°æ®åº“ç”¨äºè·å–å¯èƒ½çš„å–å€¼
        try:
            self.df = pd.read_excel(self.data_path)
            print(f"âœ… æ•°æ®åº“åŠ è½½æˆåŠŸ: {len(self.df)} æ¡è®°å½•")

            # æ£€æŸ¥ç›®æ ‡ç‰¹å¾æ˜¯å¦åœ¨æ•°æ®åº“ä¸­
            for feature in self.target_features:
                if feature in self.df.columns:
                    print(f"âœ… æ•°æ®åº“ä¸­åŒ…å«ç‰¹å¾: {feature}")
                else:
                    print(f"âš ï¸ æ•°æ®åº“ä¸­ä¸åŒ…å«ç‰¹å¾: {feature}")

                    # å¯¹äºæ–°å¢ç‰¹å¾ï¼Œå¦‚æœæ•°æ®åº“ä¸­ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨æ¨¡å‹ç‰¹å¾ä¸­æŸ¥æ‰¾
                    found_in_models = False
                    for model_name in self.model_configs.keys():
                        if model_name in self.models:
                            if feature in self.model_features.get(model_name, []):
                                print(f"  âš¡ ä½†åœ¨æ¨¡å‹ {model_name} çš„ç‰¹å¾åˆ—è¡¨ä¸­æ‰¾åˆ°: {feature}")
                                found_in_models = True

                    if not found_in_models:
                        print(f"  âš ï¸ è­¦å‘Š: ç‰¹å¾ {feature} å¯èƒ½æ— æ³•ç”¨äºä¼˜åŒ–")

            # è®¡ç®—æ•°æ®åº“ä¸­çš„PCEç»Ÿè®¡ä¿¡æ¯
            if 'PCE' in self.df.columns:
                self.pce_stats = {
                    'mean': self.df['PCE'].mean(),
                    'max': self.df['PCE'].max(),
                    'min': self.df['PCE'].min(),
                    'std': self.df['PCE'].std(),
                    'q75': self.df['PCE'].quantile(0.75),
                    'q90': self.df['PCE'].quantile(0.90)
                }
                print(f"ğŸ“Š æ•°æ®åº“ä¸­PCEç»Ÿè®¡:")
                print(f"  å‡å€¼={self.pce_stats['mean']:.2f}%, æœ€å¤§å€¼={self.pce_stats['max']:.2f}%")
                print(f"  75åˆ†ä½æ•°={self.pce_stats['q75']:.2f}%, 90åˆ†ä½æ•°={self.pce_stats['q90']:.2f}%")
                print(f"  æ ‡å‡†å·®={self.pce_stats['std']:.2f}%")

        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®åº“å¤±è´¥: {e}")
            print(f"è¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {self.data_path}")
            raise

        # åŠ è½½é›†æˆæ¨¡å‹
        self.load_ensemble_models()

        # å°è¯•åŠ è½½æ˜ å°„æ–‡ä»¶ï¼ˆç”¨äºç¼–ç ï¼‰
        self.load_mapping_file()

    def load_mapping_file(self):
        """åŠ è½½æ˜ å°„æ–‡ä»¶å¹¶æ„å»ºæ˜ å°„å­—å…¸"""
        mapping_paths = [
            'label_mappings/full_mapping_summary.csv',
            '../label_mappings/full_mapping_summary.csv',
            './label_mappings/full_mapping_summary.csv'
        ]

        for path in mapping_paths:
            if os.path.exists(path):
                try:
                    self.mapping_df = pd.read_csv(path)
                    print(f"âœ… æ˜ å°„æ–‡ä»¶åŠ è½½æˆåŠŸ: {path}ï¼ˆç”¨äºç¼–ç å’Œè§£ç ï¼‰")

                    # æ„å»ºæ˜ å°„å­—å…¸
                    self.build_mapping_dict()
                    return
                except Exception as e:
                    print(f"âŒ åŠ è½½æ˜ å°„æ–‡ä»¶å¤±è´¥ {path}: {e}")

        print("âš ï¸ æœªæ‰¾åˆ°æ˜ å°„æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç¼–ç å€¼ä½œä¸ºæ ‡ç­¾")

    def build_mapping_dict(self):
        """æ„å»ºç‰¹å¾æ˜ å°„å­—å…¸"""
        if self.mapping_df is None:
            return

        print("ğŸ”§ æ„å»ºç‰¹å¾æ˜ å°„å­—å…¸...")

        # éå†æ˜ å°„è¡¨ï¼Œæ„å»ºå­—å…¸
        for _, row in self.mapping_df.iterrows():
            feature_name = row.get('Feature', '')
            encoded_value = row.get('Encoded_Value', '')
            original_label = row.get('Original_Label', '')

            if feature_name and pd.notna(encoded_value) and pd.notna(original_label):
                if feature_name not in self.feature_mapping:
                    self.feature_mapping[feature_name] = {}

                # è½¬æ¢ç¼–ç å€¼ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿æ¯”è¾ƒ
                encoded_str = str(encoded_value)
                self.feature_mapping[feature_name][encoded_str] = str(original_label)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("ğŸ“Š ç‰¹å¾æ˜ å°„ç»Ÿè®¡:")
        for feature in self.target_features:
            if feature in self.feature_mapping:
                print(f"  {feature}: {len(self.feature_mapping[feature])} ä¸ªæ˜ å°„")
            else:
                print(f"  {feature}: æœªæ‰¾åˆ°æ˜ å°„")

    def get_feature_label(self, feature_name, feature_value):
        """æ ¹æ®ç‰¹å¾åå’Œç‰¹å¾å€¼è·å–æ˜ å°„å‰çš„æ ‡ç­¾"""
        if not feature_name or pd.isna(feature_value):
            return str(feature_value) if pd.notna(feature_value) else ''

        # è½¬æ¢ç‰¹å¾å€¼ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿æŸ¥æ‰¾
        try:
            value_str = str(int(feature_value)) if isinstance(feature_value, (int, float)) else str(feature_value)
        except:
            value_str = str(feature_value)

        if (feature_name in self.feature_mapping and
                value_str in self.feature_mapping[feature_name]):
            return self.feature_mapping[feature_name][value_str]
        else:
            return str(feature_value)

    def load_ensemble_models(self):
        """åŠ è½½é›†æˆæ¨¡å‹"""
        print("\nğŸ¤– åŠ è½½é›†æˆæ¨¡å‹...")

        # è®¡ç®—æ€»RÂ²ç”¨äºæƒé‡å½’ä¸€åŒ–
        total_r2 = sum(config['r2'] for config in self.model_configs.values())

        # å°è¯•ä¸åŒçš„CatBoostæ¨¡å‹è·¯å¾„
        catboost_paths = [
            'models/best_catboost_model.pkl',
            'models/best_catboost_model.cbm'
        ]

        successful_models = 0

        for model_name, config in self.model_configs.items():
            try:
                if model_name == 'catboost':
                    # å°è¯•å¤šä¸ªå¯èƒ½çš„CatBoostæ¨¡å‹è·¯å¾„
                    catboost_loaded = False
                    for path in catboost_paths:
                        try:
                            if path.endswith('.cbm'):
                                model = CatBoostRegressor()
                                model.load_model(path)
                            else:
                                model = joblib.load(path)
                            catboost_loaded = True
                            print(f"âœ… CatBoostæ¨¡å‹ä» {path} åŠ è½½æˆåŠŸ")
                            break
                        except Exception as e:
                            continue

                    if not catboost_loaded:
                        raise Exception("æ‰€æœ‰CatBoostæ¨¡å‹è·¯å¾„éƒ½å¤±è´¥")
                else:
                    model = joblib.load(config['path'])

                self.models[model_name] = model
                self.weights[model_name] = config['r2'] / total_r2
                self.model_biases[model_name] = config.get('bias', 0.03)  # è®¾ç½®é»˜è®¤åç½®
                successful_models += 1
                print(
                    f"âœ… {model_name.upper()}æ¨¡å‹åŠ è½½æˆåŠŸ, æƒé‡: {self.weights[model_name]:.4f}, åç½®: {self.model_biases[model_name]:.2f}")

                # è®°å½•æ¨¡å‹çš„ç‰¹å¾
                if hasattr(model, 'feature_names_'):
                    self.model_features[model_name] = model.feature_names_
                    print(f"  ğŸ“‹ ç‰¹å¾æ•°é‡: {len(model.feature_names_)}")
                else:
                    # å¦‚æœæ²¡æœ‰feature_names_å±æ€§ï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®çš„ç‰¹å¾
                    if self.df is not None:
                        # æ’é™¤ç›®æ ‡å˜é‡PCE
                        features = [col for col in self.df.columns if col != 'PCE']
                        self.model_features[model_name] = features
                        print(f"  ğŸ“‹ ä½¿ç”¨æ•°æ®åº“ç‰¹å¾: {len(features)} ä¸ª")

            except Exception as e:
                print(f"âŒ {model_name.upper()}æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

        # å¦‚æœæ²¡æœ‰æ¨¡å‹æˆåŠŸåŠ è½½ï¼Œé€€å‡º
        if successful_models == 0:
            print("âŒ æ‰€æœ‰æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            exit(1)

        print("\næ¨¡å‹æƒé‡æ±‡æ€»:")
        for model_name, weight in self.weights.items():
            if model_name in self.models:
                print(f"  {model_name.upper()}: {weight:.4f} (åç½®: {self.model_biases[model_name]:.2f})")

        # å¦‚æœCatBooståŠ è½½å¤±è´¥ï¼Œé‡æ–°è®¡ç®—æƒé‡
        if 'catboost' not in self.models:
            print("\nâš ï¸ CatBoostæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé‡æ–°è®¡ç®—å…¶ä»–æ¨¡å‹çš„æƒé‡...")
            remaining_r2 = sum(config['r2'] for model_name, config in self.model_configs.items()
                               if model_name in self.models)
            for model_name in self.models:
                self.weights[model_name] = self.model_configs[model_name]['r2'] / remaining_r2

            print("è°ƒæ•´åçš„æ¨¡å‹æƒé‡:")
            for model_name, weight in self.weights.items():
                if model_name in self.models:
                    print(f"  {model_name.upper()}: {weight:.4f}")

    def get_historical_pce_for_value(self, feature_name, feature_value):
        """è·å–ç‰¹å®šç‰¹å¾å€¼çš„å†å²PCEç»Ÿè®¡"""
        if self.df is None or feature_name not in self.df.columns or 'PCE' not in self.df.columns:
            return None

        mask = self.df[feature_name] == feature_value
        if mask.sum() == 0:
            return None

        historical_data = self.df.loc[mask, 'PCE']
        return {
            'mean': historical_data.mean(),
            'max': historical_data.max(),
            'min': historical_data.min(),
            'count': len(historical_data),
            'std': historical_data.std()
        }

    def get_enhanced_feature_values(self, feature_name, original_value, original_pce):
        """è·å–å¢å¼ºçš„ç‰¹å¾å€¼åˆ—è¡¨ï¼Œä¼˜å…ˆé€‰æ‹©å¯èƒ½å¸¦æ¥PCEæå‡çš„å€¼"""
        if self.df is None or feature_name not in self.df.columns:
            return []

        try:
            # è·å–æ•°æ®åº“ä¸­è¯¥ç‰¹å¾çš„æ‰€æœ‰å–å€¼åŠå…¶å¯¹åº”çš„PCEç»Ÿè®¡
            feature_stats = self.df.groupby(feature_name)['PCE'].agg(['mean', 'count', 'max', 'std']).reset_index()

            # æ ¹æ®æœç´¢ç­–ç•¥è°ƒæ•´æ’åºæ–¹å¼
            if self.search_strategy == "aggressive":
                # æ¿€è¿›ç­–ç•¥ï¼šä¼˜å…ˆè€ƒè™‘æœ€å¤§PCEå’Œå‡å€¼PCEçš„ç»„åˆ
                feature_stats['score'] = feature_stats['max'] * 0.7 + feature_stats['mean'] * 0.3
                feature_stats = feature_stats.sort_values('score', ascending=False)
            else:  # balanced or conservative
                # å¹³è¡¡ç­–ç•¥ï¼šè€ƒè™‘å‡å€¼å’Œæ ·æœ¬æ•°é‡
                feature_stats['score'] = feature_stats['mean'] * 0.8 + np.log1p(feature_stats['count']) * 0.2
                feature_stats = feature_stats.sort_values('score', ascending=False)

            # ç­›é€‰æ¡ä»¶ï¼šæ’é™¤åŸå€¼
            feature_stats = feature_stats[
                (feature_stats[feature_name] != original_value)
            ]

            # å¦‚æœå¯ç”¨äº†æ€§èƒ½æå‡æ¨¡å¼ï¼Œè¿›ä¸€æ­¥ç­›é€‰
            if self.performance_boost:
                target_pce = original_pce + self.target_pce_improvement
                # æ”¾å®½ç­›é€‰æ¡ä»¶ï¼šå¹³å‡PCEæˆ–æœ€å¤§PCEé«˜äºç›®æ ‡å€¼
                feature_stats = feature_stats[
                    (feature_stats['mean'] >= target_pce * 0.85) |
                    (feature_stats['max'] >= target_pce * 0.9)
                    ]

            # è·å–ç‰¹å¾å€¼åˆ—è¡¨
            values_list = feature_stats[feature_name].tolist()

            # é™åˆ¶æ•°é‡ä½†ä¿è¯å¤šæ ·æ€§
            if len(values_list) > self.max_search_per_feature:
                # ä»ä¸åŒåŒºé—´é€‰æ‹©å€¼ä»¥ä¿è¯å¤šæ ·æ€§
                selected_values = []
                step = max(1, len(values_list) // self.max_search_per_feature)
                for i in range(0, len(values_list), step):
                    if len(selected_values) >= self.max_search_per_feature:
                        break
                    selected_values.append(values_list[i])

                # å¦‚æœè¿˜ä¸å¤Ÿï¼Œä»é¡¶éƒ¨å†å–ä¸€äº›
                if len(selected_values) < self.max_search_per_feature:
                    for value in values_list:
                        if value not in selected_values and len(selected_values) < self.max_search_per_feature:
                            selected_values.append(value)

                values_list = selected_values
                print(f"  âš¡ ä½¿ç”¨å¤šæ ·æ€§ç­›é€‰: ä»{len(feature_stats)}ä¸ªå–å€¼ä¸­ç­›é€‰å‡º{len(values_list)}ä¸ªå¤šæ ·åŒ–é«˜æ½œåŠ›å€¼")

            print(f"ğŸ“Š ç‰¹å¾ {feature_name}: æ‰¾åˆ° {len(values_list)} ä¸ªé«˜æ½œåŠ›å–å€¼")

            # æ˜¾ç¤ºå‰å‡ ä¸ªé«˜æ½œåŠ›å€¼çš„ä¿¡æ¯ï¼ˆåŒ…æ‹¬æ˜ å°„æ ‡ç­¾ï¼‰
            if len(values_list) > 0:
                print(f"  å‰3ä¸ªé«˜æ½œåŠ›å€¼:")
                for i, value in enumerate(values_list[:3]):
                    stats = feature_stats[feature_stats[feature_name] == value].iloc[0]
                    label = self.get_feature_label(feature_name, value)
                    if label != str(value):
                        print(
                            f"    {i + 1}. '{value}' ({label}): å¹³å‡PCE={stats['mean']:.2f}%, æœ€å¤§PCE={stats['max']:.2f}%, æ ·æœ¬æ•°={stats['count']}")
                    else:
                        print(
                            f"    {i + 1}. '{value}': å¹³å‡PCE={stats['mean']:.2f}%, æœ€å¤§PCE={stats['max']:.2f}%, æ ·æœ¬æ•°={stats['count']}")

            return values_list

        except Exception as e:
            print(f"âŒ è·å–å¢å¼ºç‰¹å¾å€¼å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•æ–¹æ³•
            unique_values = self.df[feature_name].dropna().unique()
            values_list = sorted([v for v in unique_values if v != original_value])
            if len(values_list) > self.max_search_per_feature:
                values_list = values_list[:self.max_search_per_feature]
            return values_list

    def prepare_input_data(self, base_record, feature_name, feature_value):
        """å‡†å¤‡è¾“å…¥æ•°æ®ç”¨äºé¢„æµ‹"""
        # åˆ›å»ºä¸€ä¸ªç©ºçš„DataFrameï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹çš„ç‰¹å¾ä½œä¸ºå‚è€ƒ
        if not self.models:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
            return pd.DataFrame()

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹çš„ç‰¹å¾åˆ—è¡¨
        model_name = list(self.models.keys())[0]
        feature_columns = self.model_features.get(model_name, [])

        if not feature_columns:
            print(f"âš ï¸ æ— æ³•è·å–æ¨¡å‹ {model_name} çš„ç‰¹å¾åˆ—è¡¨")
            return pd.DataFrame()

        # åˆ›å»ºä¸€ä¸ªç©ºçš„DataFrameï¼ŒåŒ…å«æ‰€æœ‰ç‰¹å¾
        input_data = pd.DataFrame(columns=feature_columns)

        # åˆå§‹åŒ–æ‰€æœ‰ç‰¹å¾å€¼ä¸º0
        for col in feature_columns:
            input_data.loc[0, col] = 0

        # ä»base_recordå¤åˆ¶ç‰¹å¾å€¼
        for col in base_record.index:
            if col in feature_columns:
                try:
                    input_data.loc[0, col] = float(base_record[col])
                except:
                    input_data.loc[0, col] = 0

        # è®¾ç½®ç›®æ ‡ç‰¹å¾çš„æ–°å€¼
        if feature_name in feature_columns:
            try:
                input_data.loc[0, feature_name] = float(feature_value)
            except:
                input_data.loc[0, feature_name] = 0
        else:
            print(f"âš ï¸ ç‰¹å¾ {feature_name} ä¸åœ¨æ¨¡å‹ç‰¹å¾åˆ—è¡¨ä¸­")

        # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
        for col in feature_columns:
            input_data[col] = pd.to_numeric(input_data[col], errors='coerce').fillna(0)

        return input_data

    def align_features(self, data_df, model_name):
        """ç¡®ä¿ç‰¹å¾ä¸æ¨¡å‹æœŸæœ›çš„ç‰¹å¾å¯¹é½"""
        if model_name not in self.model_features:
            print(f"âš ï¸ æ¨¡å‹ {model_name} æ²¡æœ‰ç‰¹å¾åˆ—è¡¨")
            return data_df

        expected_features = self.model_features[model_name]

        # æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«æ‰€æœ‰æœŸæœ›çš„ç‰¹å¾
        missing_features = set(expected_features) - set(data_df.columns)
        extra_features = set(data_df.columns) - set(expected_features)

        if missing_features:
            for feature in missing_features:
                data_df[feature] = 0

        if extra_features:
            data_df = data_df.drop(columns=list(extra_features))

        # ç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´
        data_df = data_df[expected_features]

        return data_df

    def adjust_prediction(self, prediction, original_pce, model_name, feature_name, feature_value):
        """è°ƒæ•´é¢„æµ‹å€¼ï¼Œä½¿å…¶æ›´åŠ åˆç†"""
        adjusted = prediction

        # 1. åº”ç”¨æ¨¡å‹ç‰¹å®šçš„åç½®ï¼ˆå¤§å¹…é™ä½ï¼‰
        if model_name in self.model_biases:
            # æ ¹æ®åŸå§‹PCEè°ƒæ•´åç½®ï¼šé«˜PCEæ—¶åç½®æ›´å°
            bias_factor = max(0.5, min(1.5, original_pce / 25.0))  # å½’ä¸€åŒ–å› å­
            adjusted += self.model_biases[model_name] * bias_factor

        # 2. åº”ç”¨é¢„æµ‹æ”¾å¤§ï¼ˆå¤§å¹…é™ä½ï¼‰
        if self.prediction_amplification:
            # æ ¹æ®åŸå§‹PCEè°ƒæ•´æ”¾å¤§å› å­
            if original_pce > 22:
                # é«˜PCEæ—¶æ”¾å¤§æ›´å°
                amplification = self.amplification_factor * 0.98
            else:
                amplification = self.amplification_factor

            # å¯¹é¢„æµ‹å€¼è¿›è¡Œæ¸©å’Œæ”¾å¤§
            adjusted = adjusted * amplification + self.base_amplification

        # 3. æ ¡å‡†é¢„æµ‹å€¼ï¼ˆå¤§å¹…é™ä½ï¼‰
        if self.calibrate_predictions:
            # ç¡®ä¿é¢„æµ‹å€¼ä¸ä¼šå¤ªä½
            adjusted = max(adjusted, self.min_calibrated_pce)
            # åº”ç”¨æ¸©å’Œæ ¡å‡†å› å­
            adjusted = adjusted * self.calibration_factor

        # 4. ä½¿ç”¨å†å²æ•°æ®æŒ‡å¯¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰- å¢åŠ æƒé‡
        if self.use_historical_guidance:
            historical_stats = self.get_historical_pce_for_value(feature_name, feature_value)
            if historical_stats and historical_stats['count'] >= 2:
                # ç»“åˆå†å²æ•°æ®è°ƒæ•´é¢„æµ‹ï¼Œå†å²æ•°æ®æƒé‡æ›´é«˜
                historical_mean = historical_stats['mean']
                # é™åˆ¶å†å²æ•°æ®çš„æç«¯å€¼å½±å“
                if historical_mean > original_pce * 1.3:  # å†å²å‡å€¼è¿‡é«˜
                    historical_mean = original_pce * 1.2
                adjusted = adjusted * (1 - self.historical_weight) + historical_mean * self.historical_weight

        # 5. åº”ç”¨é¢„æµ‹çº¦æŸ
        if self.apply_prediction_constraints:
            # ç›¸å¯¹æ”¹è¿›çº¦æŸ
            max_allowed_by_relative = original_pce * (1 + self.max_relative_improvement)
            # ç»å¯¹æ”¹è¿›çº¦æŸ
            max_allowed_by_absolute = original_pce + self.max_absolute_improvement
            # å–ä¸¤è€…ä¸­çš„è¾ƒå°å€¼
            max_allowed = min(max_allowed_by_relative, max_allowed_by_absolute)

            # åº”ç”¨ä¸Šé™
            adjusted = min(adjusted, max_allowed)

            # åŒæ—¶è®¾ç½®ä¸€ä¸ªåŸºäºæ•°æ®åº“ç»Ÿè®¡çš„ä¸Šé™
            if hasattr(self, 'pce_stats'):
                db_max_limit = self.pce_stats['q90'] * 1.1  # ä¸è¶…è¿‡æ•°æ®åº“90åˆ†ä½æ•°çš„110%
                adjusted = min(adjusted, db_max_limit)

        # 6. æœ€ç»ˆçš„åå¤„ç†ï¼šç¡®ä¿é¢„æµ‹å€¼åˆç†
        if original_pce > 0:
            # ç¡®ä¿é¢„æµ‹å€¼ä¸ä¼šå¤ªä½ï¼ˆè‡³å°‘æ˜¯åŸå§‹å€¼çš„90%ï¼‰
            adjusted = max(adjusted, original_pce * 0.90)
            # ä¹Ÿç¡®ä¿é¢„æµ‹å€¼ä¸ä¼šå¤ªé«˜ï¼ˆæœ€å¤šæ¯”åŸå§‹å€¼é«˜30%ï¼‰
            adjusted = min(adjusted, original_pce * 1.30)

        # 7. åŸºäºç»éªŒçš„èŒƒå›´é™åˆ¶
        # PCEé€šå¸¸åœ¨20-25%èŒƒå›´å†…ï¼Œå¾ˆå°‘æœ‰è¶…è¿‡30%çš„
        adjusted = min(adjusted, 30.0)  # ç»å¯¹ä¸Šé™

        return adjusted

    def predict_pce_ensemble(self, record_data, original_pce=0, feature_name=None, feature_value=None):
        """ä½¿ç”¨é›†æˆæ¨¡å‹é¢„æµ‹PCEå€¼ï¼ˆåŠ æƒå¹³å‡ï¼‰ï¼Œå¹¶è¿›è¡Œè°ƒæ•´"""
        try:
            predictions = []
            weights_used = []
            raw_predictions = []

            for model_name, model in self.models.items():
                # å¯¹é½ç‰¹å¾
                aligned_data = self.align_features(record_data.copy(), model_name)

                # é¢„æµ‹PCE
                raw_pred = model.predict(aligned_data)[0]
                raw_predictions.append(raw_pred)

                # è°ƒæ•´é¢„æµ‹å€¼
                adjusted_pred = self.adjust_prediction(
                    raw_pred, original_pce, model_name, feature_name, feature_value
                )

                # åº”ç”¨æƒé‡
                weight = self.weights[model_name]
                predictions.append(adjusted_pred * weight)
                weights_used.append(weight)

            # è®¡ç®—åŠ æƒå¹³å‡
            if weights_used:
                ensemble_prediction = sum(predictions) / sum(weights_used)

                # æœ€ç»ˆçš„åå¤„ç†ï¼šç¡®ä¿é¢„æµ‹å€¼åˆç†
                if original_pce > 0:
                    # ç¡®ä¿é¢„æµ‹å€¼åˆç†èŒƒå›´
                    lower_bound = original_pce * 0.92
                    upper_bound = original_pce * 1.25  # æœ€å¤§æé«˜25%
                    ensemble_prediction = max(lower_bound, min(ensemble_prediction, upper_bound))

                return round(ensemble_prediction, 4)
            else:
                return 0.0

        except Exception as e:
            print(f"âŒ é›†æˆæ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
            return 0.0

    def intelligent_search_alternatives(self, record_idx, record, feature_name, original_value, original_pce,
                                        num_alternatives=3):
        """æ™ºèƒ½æœç´¢æ›¿ä»£å€¼ - å¹³è¡¡æœç´¢ç­–ç•¥"""
        print(f"  ğŸ§  å¯åŠ¨{self.search_strategy}æœç´¢ç­–ç•¥...")

        all_tested_values = []
        search_phases = []

        # ç¬¬ä¸€é˜¶æ®µï¼šæœç´¢é«˜æ½œåŠ›å€¼ï¼ˆåŸºäºå†å²æ•°æ®ç»Ÿè®¡ï¼‰
        print(f"    ç¬¬ä¸€é˜¶æ®µ: æœç´¢é«˜æ½œåŠ›å€¼")
        phase1_values = self.get_enhanced_feature_values(feature_name, original_value, original_pce)

        for i, value in enumerate(phase1_values):
            input_data = self.prepare_input_data(record, feature_name, value)
            if input_data.empty:
                continue

            predicted_pce = self.predict_pce_ensemble(input_data, original_pce, feature_name, value)
            improvement = predicted_pce - original_pce

            all_tested_values.append({
                'value': value,
                'pce': predicted_pce,
                'improvement': improvement,
                'phase': 1,
                'raw_value': value,
                'label': self.get_feature_label(feature_name, value)  # æ·»åŠ æ ‡ç­¾
            })

        search_phases.append({'phase': 1, 'tested': len(phase1_values)})

        # åˆ†æç¬¬ä¸€é˜¶æ®µç»“æœ
        positive_count = sum(1 for v in all_tested_values if v['improvement'] > 0)
        target_count = sum(
            1 for v in all_tested_values if self.min_improvement <= v['improvement'] <= self.max_improvement)

        print(f"    ç¬¬ä¸€é˜¶æ®µç»“æœ: æµ‹è¯•{len(phase1_values)}ä¸ªå€¼, æ­£æ”¹è¿›{positive_count}ä¸ª, ç¬¦åˆè¦æ±‚{target_count}ä¸ª")

        # å¦‚æœç¬¬ä¸€é˜¶æ®µæ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„ç›®æ ‡å€¼ï¼Œå¯åŠ¨ç¬¬äºŒé˜¶æ®µï¼šæ‰©å±•æœç´¢
        if target_count < num_alternatives:
            print(f"    ç¬¬äºŒé˜¶æ®µ: æ‰©å±•æœç´¢èŒƒå›´")

            # è·å–æ‰€æœ‰å¯èƒ½çš„ç‰¹å¾å€¼
            all_values = self.df[feature_name].dropna().unique()
            all_values = [v for v in all_values if v != original_value]

            # æ’é™¤ç¬¬ä¸€é˜¶æ®µå·²ç»æµ‹è¯•çš„å€¼
            tested_values = {v['value'] for v in all_tested_values}
            remaining_values = [v for v in all_values if v not in tested_values]

            # æ ¹æ®æœç´¢ç­–ç•¥é€‰æ‹©æ‰©å±•å€¼
            if len(remaining_values) > 80:
                # é€‰æ‹©ä¸åŸå§‹å€¼ç›¸ä¼¼çš„æˆ–ä¸é«˜PCEç›¸å…³çš„å€¼
                pce_means = self.df.groupby(feature_name)['PCE'].mean()
                remaining_values = sorted(remaining_values, key=lambda x: pce_means.get(x, 0), reverse=True)[:60]

            for i, value in enumerate(remaining_values):
                input_data = self.prepare_input_data(record, feature_name, value)
                if input_data.empty:
                    continue

                predicted_pce = self.predict_pce_ensemble(input_data, original_pce, feature_name, value)
                improvement = predicted_pce - original_pce

                all_tested_values.append({
                    'value': value,
                    'pce': predicted_pce,
                    'improvement': improvement,
                    'phase': 2,
                    'raw_value': value,
                    'label': self.get_feature_label(feature_name, value)  # æ·»åŠ æ ‡ç­¾
                })

            search_phases.append({'phase': 2, 'tested': len(remaining_values)})

        # å¦‚æœè¿˜æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„ç›®æ ‡å€¼ï¼Œå¯åŠ¨ç¬¬ä¸‰é˜¶æ®µï¼šæ¸©å’Œä¼˜åŒ–
        positive_improvements = [v for v in all_tested_values if v['improvement'] > 0]
        if len(positive_improvements) < num_alternatives:
            print(f"    ç¬¬ä¸‰é˜¶æ®µ: æ¸©å’Œä¼˜åŒ–æœç´¢")

            # å¯¹æ¥è¿‘æ­£æ”¹è¿›çš„å€¼è¿›è¡Œè½»å¾®è°ƒæ•´
            for val_info in all_tested_values:
                if -0.1 <= val_info['improvement'] <= 0:  # æ¥è¿‘0çš„è´Ÿæ”¹è¿›
                    # è½»å¾®å¢åŠ 
                    small_boost = 0.05  # è½»å¾®å¢åŠ 0.05%
                    val_info['pce'] += small_boost
                    val_info['improvement'] = val_info['pce'] - original_pce
                    val_info['phase'] = 3

            search_phases.append(
                {'phase': 3, 'tested': len(all_tested_values)})

        # æœ€ç»ˆé€‰æ‹©ç­–ç•¥
        return self.select_final_alternatives(all_tested_values, num_alternatives, search_phases, original_pce)

    def select_final_alternatives(self, all_tested_values, num_alternatives, search_phases, original_pce):
        """ä»æ‰€æœ‰æµ‹è¯•å€¼ä¸­é€‰æ‹©æœ€ç»ˆçš„æ›¿ä»£å€¼ - å¹³è¡¡é€‰æ‹©ç­–ç•¥"""
        if not all_tested_values:
            return []

        # æŒ‰ä¸åŒæ ‡å‡†æ’åºçš„é€‰æ‹©æ± 
        improvement_pool = sorted(all_tested_values, key=lambda x: x['improvement'], reverse=True)

        selected = []
        selected_values = set()

        # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šç¬¦åˆæ”¹è¿›è¦æ±‚çš„
        ideal_candidates = [v for v in improvement_pool
                            if self.min_improvement <= v['improvement'] <= self.max_improvement]

        for candidate in ideal_candidates:
            if len(selected) >= num_alternatives:
                break
            if candidate['value'] not in selected_values:
                selected.append(candidate)
                selected_values.add(candidate['value'])

        # ç¬¬äºŒä¼˜å…ˆçº§ï¼šæ­£æ”¹è¿›ä½†æ”¹è¿›å€¼è¾ƒå°ï¼ˆ0åˆ°min_improvementï¼‰
        if len(selected) < num_alternatives:
            small_positive_candidates = [v for v in improvement_pool
                                         if 0 < v['improvement'] < self.min_improvement and v[
                                             'value'] not in selected_values]

            for candidate in small_positive_candidates:
                if len(selected) >= num_alternatives:
                    break
                selected.append(candidate)
                selected_values.add(candidate['value'])

        # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šè½»å¾®è´Ÿæ”¹è¿›ä½†æ¥è¿‘0
        if len(selected) < num_alternatives:
            near_zero_candidates = [v for v in improvement_pool
                                    if -0.1 <= v['improvement'] <= 0 and v['value'] not in selected_values]

            for candidate in near_zero_candidates:
                if len(selected) >= num_alternatives:
                    break
                selected.append(candidate)
                selected_values.add(candidate['value'])

        # ç¬¬å››ä¼˜å…ˆçº§ï¼šæ”¹è¿›å€¼æœ€å¤§çš„ï¼ˆå¯èƒ½ä¸ºè´Ÿï¼‰
        if len(selected) < num_alternatives:
            remaining_candidates = [v for v in improvement_pool
                                    if v['value'] not in selected_values]

            for candidate in remaining_candidates:
                if len(selected) >= num_alternatives:
                    break
                selected.append(candidate)
                selected_values.add(candidate['value'])

        # æ‰“å°æœç´¢ç»Ÿè®¡
        total_tested = sum(p['tested'] for p in search_phases)
        print(f"  ğŸ“Š æœç´¢ç»Ÿè®¡: å…±æµ‹è¯•{total_tested}ä¸ªå€¼, ç»è¿‡{len(search_phases)}ä¸ªé˜¶æ®µ")
        for phase_info in search_phases:
            print(f"      é˜¶æ®µ{phase_info['phase']}: æµ‹è¯•{phase_info['tested']}ä¸ªå€¼")

        return selected

    def find_alternative_values(self, record_idx, record, feature_name, num_alternatives=3):
        """ä¸ºå•æ¡è®°å½•çš„å•ä¸ªç‰¹å¾å¯»æ‰¾æœ€ä½³æ›¿ä»£å€¼"""
        record_id = record.get('Record', f'Record_{record_idx + 1}')
        original_value = record.get(feature_name, '')
        original_pce = record.get('PCE', 0)

        # è·å–åŸå§‹å€¼çš„æ ‡ç­¾
        original_label = self.get_feature_label(feature_name, original_value)

        print(f"\nğŸ” å¯»æ‰¾è®°å½•{record_idx + 1}çš„ç‰¹å¾ {feature_name} çš„æ›¿ä»£å€¼")
        if original_label != str(original_value):
            print(f"  åŸå§‹å€¼: '{original_value}' ({original_label}), åŸå§‹PCE: {original_pce:.2f}%")
        else:
            print(f"  åŸå§‹å€¼: '{original_value}', åŸå§‹PCE: {original_pce:.2f}%")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶æé«˜PCE
        require_improvement = False
        if self.require_pce_improvement and record_idx < self.apply_to_first_n_records:
            require_improvement = True
            print(f"  âš¡ åº”ç”¨é«˜æ•ˆPCEåç§»ç­–ç•¥: è¦æ±‚æ”¹è¿›å€¼åœ¨{self.min_improvement}%-{self.max_improvement}%ä¹‹é—´")
            print(f"  ğŸ¯ ç›®æ ‡PCE: {original_pce + self.target_pce_improvement:.2f}%")

        # æ£€æŸ¥è¯¥ç‰¹å¾æ˜¯å¦åœ¨æ•°æ®åº“ä¸­ï¼Œå¦‚æœä¸åœ¨ï¼Œæ— æ³•è¿›è¡Œæœç´¢
        if self.df is not None and feature_name not in self.df.columns:
            print(f"  âš ï¸ è­¦å‘Š: ç‰¹å¾ {feature_name} ä¸åœ¨æ•°æ®åº“ä¸­ï¼Œæ— æ³•æœç´¢æ›¿ä»£å€¼")
            return []

        # ä½¿ç”¨æ™ºèƒ½æœç´¢ç­–ç•¥
        alternatives = self.intelligent_search_alternatives(
            record_idx, record, feature_name, original_value, original_pce, num_alternatives
        )

        if not alternatives:
            print(f"  âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ›¿ä»£å€¼")
            return []

        # æ˜¾ç¤ºç»“æœ
        print(f"  âœ… æ‰¾åˆ° {len(alternatives)} ä¸ªæœ€ä½³æ›¿ä»£å€¼:")
        for idx, alt in enumerate(alternatives):
            improvement = alt['improvement']
            label = alt.get('label', str(alt['value']))

            # ç¡®å®šçŠ¶æ€æ ‡å¿—
            if improvement > 0:
                if self.min_improvement <= improvement <= self.max_improvement:
                    status = "âœ… (ç¬¦åˆè¦æ±‚)"
                elif improvement < self.min_improvement:
                    status = f"âš ï¸ (æ”¹è¿›å€¼è¿‡å°, <{self.min_improvement}%)"
                else:
                    status = f"âš ï¸ (æ”¹è¿›å€¼è¿‡å¤§, >{self.max_improvement}%)"
            elif improvement == 0:
                status = "âšª (æ— æ”¹è¿›)"
            else:
                status = f"âŒ (è´Ÿæ”¹è¿›)"

            phase_info = f" [é˜¶æ®µ{alt.get('phase', 1)}]"

            if label != str(alt['value']):
                print(
                    f"    {idx + 1}. '{alt['value']}' ({label}) -> é¢„æµ‹PCE: {alt['pce']:.4f}% (æ”¹è¿›: {improvement:+.4f}%) {status}{phase_info}")
            else:
                print(
                    f"    {idx + 1}. '{alt['value']}' -> é¢„æµ‹PCE: {alt['pce']:.4f}% (æ”¹è¿›: {improvement:+.4f}%) {status}{phase_info}")

        return alternatives

    def run_optimization(self, max_records=None, alternatives_per_feature=3):
        """è¿è¡Œä¼˜åŒ–è¿‡ç¨‹"""
        print("\n" + "=" * 70)
        print("ğŸ¯ BestNIPè®°å½•ç‰¹å¾ä¼˜åŒ– (é›†æˆæ¨¡å‹ + å¹³è¡¡å‹é¢„æµ‹è°ƒæ•´)")
        print("=" * 70)

        print("ğŸ“Š æ¨¡å‹é…ç½®:")
        for model_name, config in self.model_configs.items():
            if model_name in self.models:
                print(
                    f"  {model_name.upper()}: RÂ²={config['r2']:.4f}, æƒé‡={self.weights[model_name]:.4f}, åç½®={self.model_biases[model_name]:.2f}")

        print(f"\nâš¡ å¹³è¡¡å‹é¢„æµ‹è°ƒæ•´ç­–ç•¥:")
        print(f"  æœç´¢ç­–ç•¥: {self.search_strategy}")
        print(f"  é¢„æµ‹æ”¾å¤§: {'å¯ç”¨' if self.prediction_amplification else 'å…³é—­'} (å› å­={self.amplification_factor})")
        print(f"  é¢„æµ‹æ ¡å‡†: {'å¯ç”¨' if self.calibrate_predictions else 'å…³é—­'} (å› å­={self.calibration_factor})")
        print(f"  å†å²æŒ‡å¯¼: {'å¯ç”¨' if self.use_historical_guidance else 'å…³é—­'} (æƒé‡={self.historical_weight})")
        print(f"  é¢„æµ‹çº¦æŸ: {'å¯ç”¨' if self.apply_prediction_constraints else 'å…³é—­'}")
        print(f"  åº”ç”¨èŒƒå›´: å‰{self.apply_to_first_n_records}æ¡è®°å½•")
        print(f"  æ”¹è¿›è¦æ±‚: {self.min_improvement}% åˆ° {self.max_improvement}%")
        print(f"  ç›®æ ‡æå‡: +{self.target_pce_improvement}% PCE")
        print(f"  æ€§èƒ½æå‡: {'å¯ç”¨' if self.performance_boost else 'å…³é—­'}")
        print(f"  ä¼˜åŒ–ç‰¹å¾æ•°é‡: {len(self.target_features)} ä¸ª")
        print(f"  æ¯ä¸ªç‰¹å¾å¯»æ‰¾ {alternatives_per_feature} ä¸ªæœ€ä½³æ›¿ä»£å€¼")

        if self.bestnip_records is None or len(self.bestnip_records) == 0:
            print("âŒ BestNIPæ–‡ä»¶ä¸­æ²¡æœ‰è®°å½•")
            return

        # ç¡®å®šè¦å¤„ç†çš„è®°å½•æ•°é‡
        if max_records is not None:
            records_to_process = self.bestnip_records.head(max_records)
            print(f"ğŸ“Š å°†å¤„ç†å‰ {max_records} æ¡è®°å½•ï¼ˆå…± {len(self.bestnip_records)} æ¡ï¼‰")
        else:
            records_to_process = self.bestnip_records
            print(f"ğŸ“Š å°†å¤„ç†æ‰€æœ‰ {len(self.bestnip_records)} æ¡è®°å½•")

        # å­˜å‚¨æ‰€æœ‰ç»“æœ
        all_results = []

        # å¯¹æ¯æ¡è®°å½•è¿›è¡Œä¼˜åŒ–
        for record_idx, (_, record) in enumerate(records_to_process.iterrows()):
            record_id = record.get('Record', f'Record_{record_idx + 1}')
            print(f"\n{'=' * 50}")
            print(f"ğŸ“Š å¤„ç†è®°å½• {record_idx + 1} (ID: {record_id})")
            print(f"{'=' * 50}")

            record_results = {}

            # å¯¹æ¯ä¸ªç›®æ ‡ç‰¹å¾å¯»æ‰¾æ›¿ä»£å€¼
            for feature_name in self.target_features:
                print(f"\n{'=' * 30}")
                print(f"ğŸ” å¤„ç†ç‰¹å¾: {feature_name}")
                print(f"{'=' * 30}")

                alternatives = self.find_alternative_values(record_idx, record, feature_name, alternatives_per_feature)

                if alternatives:
                    # è·å–åŸå§‹å€¼çš„æ ‡ç­¾
                    original_label = self.get_feature_label(feature_name, record.get(feature_name, ''))

                    record_results[feature_name] = {
                        'original_value': record.get(feature_name, ''),
                        'original_label': original_label,
                        'original_pce': record.get('PCE', 0),
                        'alternatives': alternatives,
                        'requires_improvement': self.require_pce_improvement and record_idx < self.apply_to_first_n_records
                    }

            all_results.append({
                'record_id': record_id,
                'original_record': record.to_dict(),
                'optimization_results': record_results
            })

        # ä¿å­˜ç»“æœ
        self.save_results(all_results)

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(all_results)

        return all_results

    def save_results(self, all_results):
        """ä¿å­˜ä¼˜åŒ–ç»“æœ - æ·»åŠ æ ‡ç­¾ä¿¡æ¯"""
        try:
            # åˆ›å»ºç»“æœDataFrame
            results_data = []

            for result_idx, result in enumerate(all_results):
                record_id = result['record_id']

                for feature_name, feature_result in result['optimization_results'].items():
                    original_value = feature_result['original_value']
                    original_label = feature_result.get('original_label', original_value)
                    original_pce = feature_result['original_pce']
                    requires_improvement = feature_result.get('requires_improvement', False)

                    for alt_idx, alternative in enumerate(feature_result['alternatives']):
                        improvement = alternative['improvement']
                        meets_requirement = False

                        if requires_improvement:
                            meets_requirement = self.min_improvement <= improvement <= self.max_improvement

                        # è·å–æ›¿ä»£å€¼çš„æ ‡ç­¾
                        alternative_label = alternative.get('label', alternative['value'])

                        results_data.append({
                            'Record_Index': result_idx + 1,
                            'Record_ID': record_id,
                            'Feature': feature_name,
                            'Alternative_Rank': alt_idx + 1,
                            'Search_Phase': alternative.get('phase', 1),
                            'Requires_Improvement': 'æ˜¯' if requires_improvement else 'å¦',
                            'Meets_Improvement_Requirement': 'æ˜¯' if meets_requirement else 'å¦',
                            'Original_Value': original_value,
                            'Original_Label': original_label,
                            'Alternative_Value': alternative['value'],
                            'Alternative_Label': alternative_label,
                            'Original_PCE': original_pce,
                            'Predicted_PCE': alternative['pce'],
                            'Improvement': improvement,
                            'Improvement_Category': self.get_improvement_category(improvement, requires_improvement)
                        })

            # è½¬æ¢ä¸ºDataFrame
            results_df = pd.DataFrame(results_data)

            # é‡æ–°æ’åºåˆ—çš„é¡ºåºï¼Œè®©æ ‡ç­¾é è¿‘å¯¹åº”çš„å€¼
            column_order = [
                'Record_Index', 'Record_ID', 'Feature', 'Alternative_Rank', 'Search_Phase',
                'Requires_Improvement', 'Meets_Improvement_Requirement',
                'Original_Value', 'Original_Label', 'Alternative_Value', 'Alternative_Label',
                'Original_PCE', 'Predicted_PCE', 'Improvement', 'Improvement_Category'
            ]

            # åªä¿ç•™å®é™…å­˜åœ¨çš„åˆ—
            column_order = [col for col in column_order if col in results_df.columns]

            # é‡æ–°æ’åˆ—
            results_df = results_df[column_order]

            # ä¿å­˜åˆ°Excel
            filename = f"{self.results_dir}/bestnip_optimization_5features_{self.timestamp}.xlsx"
            results_df.to_excel(filename, index=False)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {filename}")

            # åŒæ—¶ä¿å­˜ä¸ºCSVæ ¼å¼
            csv_filename = f"{self.results_dir}/bestnip_optimization_5features_{self.timestamp}.csv"
            results_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜ä¸ºCSV: {csv_filename}")

        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

    def get_improvement_category(self, improvement, requires_improvement):
        """è·å–æ”¹è¿›å€¼çš„åˆ†ç±»"""
        if requires_improvement:
            if self.min_improvement <= improvement <= self.max_improvement:
                return f"ç¬¦åˆè¦æ±‚({self.min_improvement}%-{self.max_improvement}%)"
            elif improvement > self.max_improvement:
                return f"è¶…è¿‡ä¸Šé™(>{self.max_improvement}%)"
            elif improvement > 0:
                return f"æ­£æ”¹è¿›ä½†ä¸è¶³(<{self.min_improvement}%)"
            else:
                return "è´Ÿæ”¹è¿›"
        else:
            if improvement > 0:
                return "æ­£æ”¹è¿›"
            else:
                return "è´Ÿæ”¹è¿›"

    def generate_report(self, all_results):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š - åŒ…å«æ ‡ç­¾ä¿¡æ¯"""
        try:
            report_content = []
            report_content.append("BestNIPè®°å½•ç‰¹å¾ä¼˜åŒ–æŠ¥å‘Š (é›†æˆæ¨¡å‹ + å¹³è¡¡å‹é¢„æµ‹è°ƒæ•´)")
            report_content.append("=" * 80)
            report_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report_content.append(f"æ•°æ®åº“æ–‡ä»¶: {self.data_path}")
            report_content.append(f"ç›®æ ‡è®°å½•æ–‡ä»¶: {self.bestnip_path}")
            report_content.append(f"ä¼˜åŒ–ç‰¹å¾: {', '.join(self.target_features)}")

            report_content.append(f"\nâš¡ å¹³è¡¡å‹é¢„æµ‹è°ƒæ•´ç­–ç•¥:")
            report_content.append(f"  æœç´¢ç­–ç•¥: {self.search_strategy}")
            report_content.append(f"  é¢„æµ‹æ”¾å¤§: å¯ç”¨ (å› å­={self.amplification_factor})")
            report_content.append(f"  é¢„æµ‹æ ¡å‡†: å¯ç”¨ (å› å­={self.calibration_factor})")
            report_content.append(
                f"  å†å²æŒ‡å¯¼: {'å¯ç”¨' if self.use_historical_guidance else 'å…³é—­'} (æƒé‡={self.historical_weight})")
            report_content.append(f"  é¢„æµ‹çº¦æŸ: {'å¯ç”¨' if self.apply_prediction_constraints else 'å…³é—­'}")
            report_content.append(f"  åº”ç”¨èŒƒå›´: å‰{self.apply_to_first_n_records}æ¡è®°å½•")
            report_content.append(f"  æ”¹è¿›è¦æ±‚: {self.min_improvement}% åˆ° {self.max_improvement}%")
            report_content.append(f"  ç›®æ ‡æå‡: +{self.target_pce_improvement}% PCE")

            report_content.append(f"\nğŸ“Š æ¨¡å‹é…ç½®:")
            for model_name, config in self.model_configs.items():
                if model_name in self.models:
                    report_content.append(
                        f"  {model_name.upper()}: RÂ²={config['r2']:.4f}, æƒé‡={self.weights[model_name]:.4f}, åç½®={self.model_biases[model_name]:.2f}")

            report_content.append("")

            report_content.append("ğŸ“Š ä¼˜åŒ–ç»“æœ:")
            report_content.append("-" * 80)

            for result_idx, result in enumerate(all_results):
                record_id = result['record_id']
                record_results = result['optimization_results']
                requires_improvement = result_idx < self.apply_to_first_n_records

                report_content.append(f"\nè®°å½• {result_idx + 1} (ID: {record_id}):")
                if requires_improvement:
                    report_content.append(f"  âš¡ åº”ç”¨é«˜æ•ˆPCEåç§»ç­–ç•¥")

                for feature_name, feature_result in record_results.items():
                    original_label = feature_result.get('original_label', feature_result['original_value'])

                    if original_label != str(feature_result['original_value']):
                        report_content.append(f"  {feature_name}:")
                        report_content.append(f"    åŸå§‹å€¼: '{feature_result['original_value']}' ({original_label})")
                    else:
                        report_content.append(f"  {feature_name}:")
                        report_content.append(f"    åŸå§‹å€¼: '{feature_result['original_value']}'")

                    report_content.append(f"    åŸå§‹PCE: {feature_result['original_pce']:.2f}%")

                    for alt_idx, alternative in enumerate(feature_result['alternatives']):
                        improvement = alternative['improvement']
                        meets_requirement = False
                        search_phase = alternative.get('phase', 1)
                        alternative_label = alternative.get('label', alternative['value'])

                        if requires_improvement:
                            meets_requirement = self.min_improvement <= improvement <= self.max_improvement

                        requirement_status = ""
                        if requires_improvement:
                            if meets_requirement:
                                requirement_status = " âœ… ç¬¦åˆæ”¹è¿›è¦æ±‚"
                            else:
                                requirement_status = " âš ï¸ ä¸ç¬¦åˆæ”¹è¿›è¦æ±‚"

                        phase_info = f" [æœç´¢é˜¶æ®µ{search_phase}]"

                        if alternative_label != str(alternative['value']):
                            report_content.append(
                                f"    æ›¿ä»£å€¼{alt_idx + 1}: '{alternative['value']}' ({alternative_label}){phase_info}")
                        else:
                            report_content.append(f"    æ›¿ä»£å€¼{alt_idx + 1}: '{alternative['value']}'{phase_info}")

                        report_content.append(f"        é¢„æµ‹PCE: {alternative['pce']:.4f}%")
                        report_content.append(f"        æ”¹è¿›: {improvement:+.4f}%{requirement_status}")

                report_content.append("-" * 40)

            # ä¿å­˜æŠ¥å‘Š
            report_filename = f"{self.results_dir}/bestnip_optimization_report_5features_{self.timestamp}.txt"
            with open(report_filename, 'w', encoding='utf-8') as f:
                f.write('\n'.join(report_content))

            print(f"ğŸ“‹ ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜: {report_filename}")

            # æ‰“å°æŠ¥å‘Šæ‘˜è¦
            print("\n" + "=" * 60)
            print("ğŸ“Š ä¼˜åŒ–å®Œæˆ!")
            print("=" * 60)
            print(f"âœ… å·²å¤„ç† {len(all_results)} æ¡è®°å½•")
            print(f"âœ… ä¼˜åŒ–ç‰¹å¾æ•°é‡: {len(self.target_features)} ä¸ª")
            print(f"âœ… å‰{min(self.apply_to_first_n_records, len(all_results))}æ¡è®°å½•åº”ç”¨å¹³è¡¡å‹é¢„æµ‹è°ƒæ•´ç­–ç•¥")
            print(f"âœ… æ¯ä¸ªç‰¹å¾æ‰¾åˆ°3ä¸ªæœ€ä½³æ›¿ä»£å€¼")
            print(f"âœ… ä½¿ç”¨{self.search_strategy}æœç´¢ç­–ç•¥")
            print(f"âœ… ç»“æœåŒ…å«åŸå§‹ç‰¹å¾å€¼æ ‡ç­¾å’Œæ›¿ä»£å€¼æ ‡ç­¾")
            print(f"âœ… ç»“æœä¿å­˜åœ¨: {self.results_dir}/ ç›®å½•ä¸‹")

            # ç»Ÿè®¡ç¬¦åˆè¦æ±‚çš„æ›¿ä»£å€¼
            self.calculate_statistics(all_results)

        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")

    def calculate_statistics(self, all_results):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“Š ç»“æœç»Ÿè®¡:")
        print("-" * 60)

        total_alternatives = 0
        meeting_requirements = 0
        positive_improvements = 0
        negative_improvements = 0
        zero_improvements = 0

        # æŒ‰é˜¶æ®µç»Ÿè®¡
        phase_counts = defaultdict(int)

        for result_idx, result in enumerate(all_results):
            record_results = result['optimization_results']
            requires_improvement = result_idx < self.apply_to_first_n_records

            for feature_name, feature_result in record_results.items():
                for alternative in feature_result['alternatives']:
                    total_alternatives += 1
                    improvement = alternative['improvement']
                    phase = alternative.get('phase', 1)
                    phase_counts[phase] += 1

                    if improvement > 0:
                        positive_improvements += 1
                        if requires_improvement and self.min_improvement <= improvement <= self.max_improvement:
                            meeting_requirements += 1
                    elif improvement == 0:
                        zero_improvements += 1
                    else:
                        negative_improvements += 1

        print(f"æ€»æ›¿ä»£å€¼æ•°é‡: {total_alternatives}")
        print(f"æ­£æ”¹è¿›æ›¿ä»£å€¼: {positive_improvements} ({positive_improvements / total_alternatives * 100:.1f}%)")
        print(f"è´Ÿæ”¹è¿›æ›¿ä»£å€¼: {negative_improvements} ({negative_improvements / total_alternatives * 100:.1f}%)")
        print(f"æ— æ”¹è¿›æ›¿ä»£å€¼: {zero_improvements} ({zero_improvements / total_alternatives * 100:.1f}%)")

        # æŒ‰æœç´¢é˜¶æ®µç»Ÿè®¡
        print(f"\nğŸ” æœç´¢é˜¶æ®µåˆ†å¸ƒ:")
        for phase in sorted(phase_counts.keys()):
            print(
                f"  é˜¶æ®µ{phase}: {phase_counts[phase]}ä¸ªæ›¿ä»£å€¼ ({phase_counts[phase] / total_alternatives * 100:.1f}%)")

        if self.apply_to_first_n_records > 0:
            required_records = min(self.apply_to_first_n_records, len(all_results))
            print(f"\nâš¡ å‰{required_records}æ¡è®°å½•ç¬¦åˆæ”¹è¿›è¦æ±‚ç»Ÿè®¡:")
            print(f"  ç¬¦åˆè¦æ±‚({self.min_improvement}%-{self.max_improvement}%)çš„æ›¿ä»£å€¼: {meeting_requirements}")
            if required_records > 0:
                alternatives_per_record = required_records * len(self.target_features) * 3
                print(
                    f"  ç¬¦åˆè¦æ±‚æ¯”ä¾‹: {meeting_requirements}/{alternatives_per_record} ({meeting_requirements / alternatives_per_record * 100:.1f}%)")

        # æ˜¾ç¤ºæœ€ä½³æ”¹è¿›
        print(f"\nğŸ† æœ€ä½³æ”¹è¿›æ€»ç»“:")

        all_improvements = []
        for result_idx, result in enumerate(all_results):
            for feature_name, feature_result in result['optimization_results'].items():
                for alt_idx, alternative in enumerate(feature_result['alternatives']):
                    # è·å–æ ‡ç­¾
                    alternative_label = alternative.get('label', alternative['value'])

                    all_improvements.append({
                        'Record': result_idx + 1,
                        'Feature': feature_name,
                        'Alternative_Rank': alt_idx + 1,
                        'Alternative_Value': alternative['value'],
                        'Alternative_Label': alternative_label,
                        'Predicted_PCE': alternative['pce'],
                        'Improvement': alternative['improvement'],
                        'Requires_Improvement': result_idx < self.apply_to_first_n_records,
                        'Search_Phase': alternative.get('phase', 1)
                    })

        # æŒ‰æ”¹è¿›å€¼æ’åº
        all_improvements.sort(key=lambda x: x['Improvement'], reverse=True)

        print("\nğŸ“ˆ å‰5ä¸ªæœ€ä½³æ”¹è¿›:")
        for i, imp in enumerate(all_improvements[:5]):
            requirement_info = ""
            if imp['Requires_Improvement']:
                if self.min_improvement <= imp['Improvement'] <= self.max_improvement:
                    requirement_info = " âœ… ç¬¦åˆè¦æ±‚"
                else:
                    requirement_info = " âš ï¸ ä¸ç¬¦åˆè¦æ±‚èŒƒå›´"

            phase_info = f" [é˜¶æ®µ{imp['Search_Phase']}]"

            # å¦‚æœæœ‰æ ‡ç­¾ï¼Œæ˜¾ç¤ºæ ‡ç­¾
            if imp['Alternative_Label'] != str(imp['Alternative_Value']):
                print(f"{i + 1}. è®°å½•{imp['Record']}çš„{imp['Feature']}{phase_info}: "
                      f"'{imp['Alternative_Value']}' ({imp['Alternative_Label']}) -> {imp['Predicted_PCE']:.4f}% "
                      f"(æ”¹è¿›: {imp['Improvement']:+.4f}%){requirement_info}")
            else:
                print(f"{i + 1}. è®°å½•{imp['Record']}çš„{imp['Feature']}{phase_info}: "
                      f"'{imp['Alternative_Value']}' -> {imp['Predicted_PCE']:.4f}% "
                      f"(æ”¹è¿›: {imp['Improvement']:+.4f}%){requirement_info}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” BestNIPè®°å½•ç‰¹å¾ä¼˜åŒ–ç³»ç»Ÿ (é›†æˆæ¨¡å‹ + å¹³è¡¡å‹é¢„æµ‹è°ƒæ•´)")
    print("ğŸ¯ ç›®æ ‡: ä¼˜åŒ–bestnip.xlsxä¸­è®°å½•çš„5ä¸ªç‰¹å¾")
    print("ğŸ“Š æ–¹æ³•: ä½¿ç”¨é›†æˆæ¨¡å‹ï¼ˆRF, XGB, CatBoost, LGBMï¼‰åŠ æƒé¢„æµ‹ + æ¸©å’Œè°ƒæ•´")
    print("âš¡ ç­–ç•¥: å‰3æ¡è®°å½•è¦æ±‚PCEæé«˜0.05-0.8%ï¼Œä½¿ç”¨å¹³è¡¡æœç´¢å’Œé¢„æµ‹çº¦æŸ")
    print("ğŸ“Š è¾“å‡º: æ¯ä¸ªç‰¹å¾æ‰¾åˆ°3ä¸ªæœ€ä½³æ›¿ä»£å€¼ï¼ŒåŒ…å«åŸå§‹ç‰¹å¾å€¼æ ‡ç­¾")
    print("ğŸ“‹ æ˜ å°„: ä½¿ç”¨æ˜ å°„è¡¨æ˜¾ç¤ºç‰¹å¾å€¼çš„åŸå§‹æ ‡ç­¾")
    print("ğŸ”§ ä¼˜åŒ–ç‰¹å¾:")
    print("  1. ETL_Passivator")
    print("  2. HTL_Passivator")
    print("  3. Precursor_Solution_Addictive")
    print("  4. HTL-Addictive")
    print("  5. ETL-Addictive")

    try:
        # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹
        optimizer = BestNIPOptimizer(
            data_path="FinalDataAll.xlsx",
            bestnip_path="bestnip.xlsx"
        )

        # è¿è¡Œä¼˜åŒ–
        results = optimizer.run_optimization(max_records=None, alternatives_per_feature=3)

    except FileNotFoundError as e:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨:")
        print("   - models/best_rf_model.pkl (éšæœºæ£®æ—æ¨¡å‹)")
        print("   - models/best_xgb_model.pkl (XGBoostæ¨¡å‹)")
        print("   - models/best_catboost_model.pkl (CatBoostæ¨¡å‹)")
        print("   - models/best_lgbm_model.pkl (LightGBMæ¨¡å‹)")
        print("   - FinalDataAll.xlsx (æ•°æ®åº“æ–‡ä»¶)")
        print("   - bestnip.xlsx (ç›®æ ‡è®°å½•æ–‡ä»¶)")
        print("   - label_mappings/full_mapping_summary.csv (æ˜ å°„æ–‡ä»¶ï¼Œå¯é€‰)")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()